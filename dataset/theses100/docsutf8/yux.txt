Department of Computer Science

Hamilton, NewZealand

Prediction Intervals for Class Probabilities
Xiaofeng Yu

This thesis is submitted in partial fulfilment of the requirements for the degree of Master of Science at The University of Waikato.

January 2007 c 2007 Xiaofeng Yu

Abstract
Prediction intervals for class probabilities are of interest in machine learning because they can quantify the uncertainty about the class probability estimate for a test instance. The idea is that all likely class probability values of the test instance are included, with a pre-specified confidence level, in the calculated prediction interval. This thesis proposes a probabilistic model for calculating such prediction intervals. Given the unobservability of class probabilities, a Bayesian approach is employed to derive a complete distribution of the class probability of a test instance based on a set of class observations of training instances in the neighbourhood of the test instance. A random decision tree ensemble learning algorithm is also proposed, whose prediction output constitutes the neighbourhood that is used by the Bayesian model to produce a PI for the test instance. The Bayesian model, which is used in conjunction with the ensemble learning algorithm and the standard nearest-neighbour classifier, is evaluated on artificial datasets and modified real datasets.

i

Acknowledgments
First of all, I am deeply indebted to my supervisor Dr. Eibe Frank, for the time he has spent on the thesis, for his invaluable knowledge, great patience, and consistent encouragement throughout my study. Without him, there wouldn't be this thesis! He taught me how to approach a research problem in different ways and find a breakthrough. He showed me by setting an example the need to be persistent and have a never-give-up attitude in order to achieve a success. He was always there to listen and give advice. He would repeat explaining the same question until I finally understood, and I will always be grateful for that.

There are many people in the Machine Learning group, who have been very supportive of my study. I first thank Ashraf Kibriya for helping me with configuring Weka Experimenter. I would like to thank Dr. Bernhard Pfahringer for helping me interpret the Fortune code. I would also like to thank Peter Reutemann, Richard Kirkby, Gabi Schmidberger, Stefan Mutter, Haijian Shi, Grant Anderson, and Lin Dong, and many others in the group, for all the help they have given to me.

I thank Dr.

Bill Bolstad from Statistics Department for the copy of WinBUGS

manual, and my friend Quan Qiu from the Digital Library group for the LaTeX editor.

Finally, I thank my wife, Shaoqun, who has always been supportive during my study by taking over all the household chores. It'll soon be over :-)

iii

Contents
Abstract Acknowledgments 1 Introduction 1.1 1.2 1.3 1.4 A Medical Diagnosis Example . . . . . . . . . . . . . . . . . . . . . . . . . . Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Aims of the Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Thesis Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . i iii 1 1 3 4 5 7 7 7 9

2 Background 2.1 Bayesian Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.1 2.1.2 2.1.3 2.2 The Bayesian Framework . . . . . . . . . . . . . . . . . . . . . . . . Model Specification . . . . . . . . . . . . . . . . . . . . . . . . . . .

Numerical Computation . . . . . . . . . . . . . . . . . . . . . . . . . 12

Statistical Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.2.1 2.2.2 2.2.3 2.2.4 2.2.5 Point Versus Interval Estimation . . . . . . . . . . . . . . . . . . . . 14 Types of Statistical Intervals . . . . . . . . . . . . . . . . . . . . . . 16 Why Do We Need PIs . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Assumptions in Interval Calculation . . . . . . . . . . . . . . . . . . 18 Distribution-free Statistical Intervals . . . . . . . . . . . . . . . . . . 21

2.3

Classification Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.3.1 2.3.2 2.3.3 Regression Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Nearest-Neighbour Classification . . . . . . . . . . . . . . . . . . . . 25 Ensemble of Classifiers . . . . . . . . . . . . . . . . . . . . . . . . . . 26

2.4

Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 29

3 Literature Review 3.1 3.2 3.3

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 PI Computation Using Theoretical Formulae . . . . . . . . . . . . . . . . . 29 Empirically based PIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 v

3.4 3.5 3.6 3.7

PI Methods by Simulation and Resampling . . . . . . . . . . . . . . . . . . 33 Distribution-free PIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 PIs based on the Bayesian Approach . . . . . . . . . . . . . . . . . . . . . . 36 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 39

4 Methodology 4.1 4.2 4.3 4.4 4.5 4.6 4.7

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 General Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 Parametric Modelling Framework . . . . . . . . . . . . . . . . . . . . . . . . 41 Modelling Class Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . 42 PI Computation by Model Simulation . . . . . . . . . . . . . . . . . . . . . 46 A Random Tree Ensemble Classifier . . . . . . . . . . . . . . . . . . . . . . 53 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 57

5 Evaluation 5.1 5.2

Classifier Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 Evaluation of the Bayesian PI Model . . . . . . . . . . . . . . . . . . . . . . 61 5.2.1 5.2.2 5.2.3 5.2.4 Experimental Setup for the Bayesian PI Model . . . . . . . . . . . . 62 Testing with Beta Random Numbers . . . . . . . . . . . . . . . . . . 63 Testing with Artificial Datasets . . . . . . . . . . . . . . . . . . . . . 69 Testing with Semi-Real Datasets . . . . . . . . . . . . . . . . . . . . 74

5.3

Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 81

6 Conclusions and Future Work 6.1 6.2

Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 87

Appendix A

vi

List of Figures
1.1 2.1 3.1 Illustration showing a prediction interval (PI). . . . . . . . . . . . . . . . . . 4

Comparison of widths of statistical intervals for the same example. . . . . . 17 The new observation xn+1 could occupy any of the n + 1 positions formed by the n ordered observations. . . . . . . . . . . . . . . . . . . . . . . . . . 35

4.1 4.2

Examples of beta densities. . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 (a) p(PL  Pn+1  PU ) = the area under the density curve between PL and PU . (b) Smooth curve area is simulated by summation of a large number of rectangles. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

4.3

Different priors come with different sampling spaces in relation to the computation of the posterior distribution. (a) The straight line forms the sampling space for the prior  +  = 10. (b) The triangular area under the line  +  = 10 forms the sampling space for the prior U(0, 10). (c) The whole square area forms the sampling space for the prior gamma(1, 1). . . . . . . 51

4.4

A U-shaped symmetric posterior density graph for which the 95% PI is computed in two different ways: (a) a central joint PI; (b) a disjoint PI with the narrowest width. A skewed J-shaped posterior density curve, for which the calculated 95% PI is computed: (c) a central PI; (d) the narrowest PI becomes an one-sided interval bound. . . . . . . . . . . . . . . . . . . . . 52

4.5

Illustration of the random tree ensemble inducting and prediction process. The letters A to K represent 10 training instances in the training dataset. The superscript denotes the corresponding instance's class. The induced ensemble consists of four single trees, in which splitting nodes are represented by circles and leaves by rectangles. In this example, Leaf-1 to Leaf-4 are used to form the neighbourhood for a test instance. . . . . . . . . . . . 53

5.1

Prediction accuracy comparison between EnsembleRT and Knear. EnsembleRT is better than Knear on datasets D6, D7, and D9; Knear is more accurate than EnsembleRT on D5 and D8. . . . . . . . . . . . . . . . . . . . 60 vii

5.2

Prediction accuracy comparison between KnearEnsembleRT and Knear. Only on one dataset (D6) do the two have significant difference, where KnearEnsembleRT outperforms Knear. . . . . . . . . . . . . . . . . . . . . . 60

5.3

Prediction accuracy comparison between EnsembleRT and KnearEnsembleRT. EnsembleRT and KnearEnsembleRT outperform each other on the datasets D7, D9 and D5, D8, respectively. . . . . . . . . . . . . . . . . . . . 60

5.4

Depiction of PIs and their targets (the

) on eight trials. The number on

the PI line is the width of the PI. In 100L% of the trials the value of the target should lie in the computed PI. The numbers marked on the lines are the widths of the PIs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 5.5 Illustration of the localised instances (the oval for EnsembleRT, and the small rounded rectangles for the other two), based on which the prediction is made. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 5.6 Comparison of the PICPs (the four splines in the upper part) and the average widths of the PIs (the three splines in the lower part), computed at various confidence levels. The number of trials at each level is 1000. The red spline with upward triangles represents the theoretical (ideal) PICPs, i.e. the corresponding confidence levels. . . . . . . . . . . . . . . . . . . . . 66 5.7 Comparison of the highest and lowest widths of the PIs, computed at various confidence levels. The number of trials is also 1000. . . . . . . . . . . . . . . 66 5.8 Experiment results of 10,000 trials. The PIs were computed with the 95% confidence level. (a) The trend of PICP, corresponding to 1000, 5000, and 10,000 trials. (b) The trend of average width. (c) The comparison of the avk  erage value of ( n - + ) between the successful trials (marked as 'capture')

and those failed (marked as 'miss'). . . . . . . . . . . . . . . . . . . . . . . . 66 5.9 Results of experiments computing the PICPs and the widths of the PIs when systematically varying the union size with different sampling priors for the beta parameters. The sampled beta parameter values are: (a)  = 1.93,  = 0.14; (b)  = 4.82,  = 7.98; (c)  = 9.64,  = 0.36. . . . . . . . . . . . 69 5.10 Illustration of the distributions of the two classes of a generated artificial dataset with a single attribute. . . . . . . . . . . . . . . . . . . . . . . . . . 73 5.11 Experiment results for classifier EnsembleRT on artificial datasets created with two different class proportions. . . . . . . . . . . . . . . . . . . . . . . 75 5.12 Experiment results for classifier Knear on artificial datasets created with two different class proportions. . . . . . . . . . . . . . . . . . . . . . . . . . 75 viii

5.13 Experiment results for classifier KnearEnsembleRT on artificial datasets created with two different class proportions. . . . . . . . . . . . . . . . . . . 75 5.14 Comparison between datasets with linear and nonlinear class spaces. (a) ­ (c) for EnsembleRT; (d) ­ (f) for Knear; (g) ­ (i) for KnearEnsembleRT. . . 77 5.15 PICP and average width at various confidence levels. The classifier used was EnsembleRT, and the dataset was ionosphere.arff. . . . . . . . . . . . . 77 5.16 Comparison of PICP, average width, and prediction accuracy when normalising and not normalising the attribute values of the dataset. (a) ­ (c): classifier Knear on linear dataset, and (d) ­ (f): Knear on nonlinear dataset; (g) ­ (i): classifier KnearEnsembleRT on linear dataset, and (j) ­ (l): KnearEnsembleRT on nonlinear dataset. . . . . . . . . . . . . . . . . . 78

ix

List of Tables
5.1 5.2 5.3 Available options for the three classifiers. . . . . . . . . . . . . . . . . . . . 58 Datasets used in the experiments. . . . . . . . . . . . . . . . . . . . . . . . . 58 Option values tested with the three classifiers, from which the optimum values were selected. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 5.4 Optimum option values selected for the three classifiers on the nine datasets tested in the experiments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 5.5 The class proportions of the artificial datasets generated using equal prior probability (the third column), and based on the class proportions of the real datasets listed in Table 5.2 (the fourth column). . . . . . . . . . . . . . 76

xi

Chapter 1 Introduction
From helping to perform critical medical diagnosis to forecasting of destructive natural phenomena, decision makers are frequently faced with the necessity of obtaining not only accurate predictions but also uncertainty estimates associated with the predictions. Machine learning technologies have become important tools desired by decision makers to make accurate and reliable predictions across various fields. However, these techniques generally fail to quantify predictive uncertainty: the prediction is a single number, and it does not provide any information about how likely the number is the desired 'true' value.

When using machine learning techniques to perform categorical data analysis, the output normally takes the form of a classification, often with a probability estimate. The estimated probability indicates, in conjunction with the classification of the training data, the degree of belief that an unknown example belongs to the predicted class. A commonly used approach to obtaining a classification and the corresponding probability estimate is to take a majority vote to obtain the classification and then compute the proportion of interest among all the observations in order to estimate the class probability. Instead of simply providing a probability estimate for the class of an unknown example, this thesis attempts to quantify the uncertainty associated with the class probability estimate, as the estimate is usually based on limited data.

1.1

A Medical Diagnosis Example

Using machine learning algorithms for the analysis of medical data has a long history. Today, technologies derived from these algorithms are well suited for specialised diagnosis problems. Many modern hospitals are equipped with expert systems built upon machine learning techniques, and patients are often diagnosed by these 'smart' programs before being actually admitted by human physicians (Kononenko, 2002, p. 2). Suppose a

potential cancer patient comes to a hospital. Based on her or his symptoms, searching a database of records of previous cancer patients gives a result of five similar cases, of 1

which four were diagnosed with cancer. Taking a majority vote gives the diagnosis that the patient has developed cancer, and the estimated probability that this diagnosis is correct is 0.8 (four out of five).

Before asking the patient to take any further tests, can the physician be sure, based on the patient's specific symptoms, that 0.8 is the 'true' probability that she or he is having cancer? The answer to this question depends on the similarity between this patient's symptoms and the symptoms of the five previous patients, on which the diagnosis was based. It also depends on the amount of data (i.e. the number of patient records) that was involved in the diagnosis and used to come up with the estimated probability, 0.8. In fact, this probability value (0.8) is just an estimate of the likelihood that any patient, who has a set of symptoms similar to that of the patient currently being diagnosed, has developed cancer. This is not what the physician really wants. Ideally, the physician would like a probability, and an interval estimate, that are estimated based on the records of a group of patients with the exact symptoms of the current patient. If such data were available, the task of predicting the desired class probability, along with an interval estimate, would be easily accomplished by many existing techniques. Unfortunately, in practice, we are unlikely to observe different patients with known diagnoses who present exactly the same symptoms. Therefore probabilities and intervals of this sort cannot be directly estimated, and straightforward application of standard methods is not possible.

All we can do with machine learning is use the records from patients with similar symptoms in order to diagnose a new patient. We have noticed that the new patient is still sharing at least some of the symptoms with each of the previous patients with known diagnoses. Thus, we first use these common symptoms to select a group of patients with symptoms similar to that of the new patient, resulting in a set of values that are either 1 or 0, where 1 means cancer and 0 means not cancer. Then we derive a Bayesian model, which takes as input this set of values (i.e. 0s and 1s) to produce an entire range of plausible probabilities that contain, with a specified degree of confidence, the 'true' value of probability of the new patient having cancer. Therefore, we can use the width of the computed probability range to predict what the chances of the patient having cancer really are. If the obtained range is too wide (such as one near 1.0), which indicates the estimate is too imprecise, we may have to ask the patient to take further tests before making the final diagnosis.

2

A question that may arise at this point is that a patient either has developed cancer or has not, which means the 'true' probability value for the patient is either 1 or 0. How can we say a probability takes on a value somewhere between 0 and 1? The explanation for this is that we do not have complete knowledge about the physical conditions of the patient. Here, by 'complete knowledge', we mean that anything and everything that accompanies the disease and is regarded as an indication of existence of the disease must be known. In this sense, our limited knowledge, in the form of the symptoms we have observed on the patient, leads to an estimated probability value between 0 and 1, rather than just the two possible values 0 or 1.

1.2

Prediction Intervals

In statistics, an interval like the one described above is called prediction interval (henceforth abbreviated PI), which is computed to contain an unknown quantity with a specified confidence level. In the above discussed medical diagnosis example, the

unknown quantity is the probability of a patient having cancer. Applying the medical example in the machine learning context, the unknown quantity is considered to be the class probability of an unknown instance. A PI is usually comprised of an upper and a lower limit, between which a future unknown value is expected to lie with a prescribed confidence level. The future value can be something that is observable, such as a person's height. Or it may be unobservable, such as the probability value in the cancer patient example.

Figure 1.1 illustrates the concept of PI. Note that the calculated vertical PI line forms a joint continuous range. In fact, a PI could comprise two (or may be more) disjoint intervals, in which case, the term prediction regions may be more appropriate. Regardless of this, the desired 'true' probability value must not, with the pre-specified degree of confidence, fall outside the interval(s). The width of the computed PI can thus be used to quantify the uncertainty about the probability prediction of how likely the estimated probability is the 'true' probability.

Despite being rather neglected (Chatfield, 1998), there have been several approaches to computing PI proposed by researchers. Unfortunately, none of the methods derived from these approaches were designed, nor are they suitable for solving the problem in our situation. The reason for this is that those PIs are computed for either an observable 3

1.0 Probability of Cancer

6
Upper Limit:

PU 

6

  Unobservable True Probability ·  Predicted Probability

Computed PI

Lower Limit:

? PL 

0.0

?

-

A new patient to be diagnosed

Symptom values

Figure 1.1: Illustration showing a prediction interval (PI).

unknown quantity, or the observable properties of an unknown quantity, whereas we want to compute PIs for class probabilities, which cannot be directly observed.

1.3

Aims of the Study

In this study, we restrict our attention to computing PIs for the class probability of a single unknown instance, not a set of unknown instances. We also consider only

classification problems with dichotomous outcome, and do not consider the more difficult multi-class problems.

We begin with deriving a model, using the Bayesian approach. The model can be used to compute a PI for the class probability of an unknown instance, based on a set of previous instances with known classes. Then we introduce a random decision tree ensemble learning algorithm, based on which a classifier is built to produce a group of similar training instances (e.g. the instances in a leaf node of a tree). These selected instances are then used to compute an estimated class probability for an unknown instance. The class labels of those selected training instances are used as input in the proposed Bayesian probabilistic model to produce a PI for the class probability of the tested new instance. The calculated PI is supposed to capture the 'true' class probability of the unknown instance.

We conduct experiments with the aim of testing how well the Bayesian PI model performs in terms of the percentage that the PIs capture the 'true' class probabilities, and the narrowness of the widths of the computed PIs. The proposed decision tree learner is 4

evaluated by the measurement of root-mean-squared-error (RMSE), which is used to measure the accuracy of the generated probability estimates. The results of the experiments show that, (a) the decision tree learner has prediction performance comparable with the k-nearest-neighbour classifier, and (b) the capture percentage of the PI calculation model reaches the specified confidence level while still maintaining reasonably narrow widths.

1.4

Thesis Structure

The rest of the thesis is organised as follows. In Chapter 2, we first introduce some background on Bayesian data analysis. Then we review the basic concepts that are useful for understanding statistical intervals. We also discuss with examples three commonly used types of statistical intervals. Finally, we discuss three classification models.

Chapter 3 conducts a brief survey of literature regarding the construction of PIs. We categorise the methods of calculating PIs according to various approaches, based on which different methods have been derived. We give examples of the methods and point out the advantages and disadvantages.

The proposed Bayesian probabilistic model derived within a parametric modelling framework is developed in Chapter 4. We present the reasoning behind the mathematical form of the proposed model, and the computation of the resulting distribution. Issues such as the assumptions made during the model derivation process are also discussed. A random decision tree ensemble learning algorithm is also proposed in this chapter.

In Chapter 5, experiments are performed to evaluate the proposed ensemble learning algorithm and the Bayesian PI model. In the evaluation, we use hypothetical instances, artificially generated datasets, and datasets selected from the UCI repository of machine learning datasets. We analyse and discuss the results from the experiments on respective datasets.

The thesis is concluded in Chapter 6. We summarise the main findings of the study. We discuss the advantages and limitations of the proposed model and prioritise issues that require further investigation.

5

Chapter 2 Background
This chapter is devoted to material that is useful for understanding what will be discussed in subsequent chapters. We begin in Section 2.1 with a introduction to the basic concepts of Bayesian data analysis. We describe the core idea behind Bayesian thinking: updating prior knowledge about an unknown quantity with the observed data to arrive at the desired posterior distribution. This is followed by a description of the main tasks involved in the Bayesian learning process such as specifying the prior probability model. We also discuss some of the commonly used numerical methods for posterior computation. Section 2.2 introduces the basic concepts of statistical intervals. Three types of statistical intervals are described. We point out the situations in which each should be used. Section 2.3 discusses several relevant classification models, including the nearest-neighbour classifier, the linear and logistic regression models, and some ensemble learning approaches that utilise a combination of several models. We summarise the chapter in Section 2.4.

2.1

Bayesian Data Analysis

Driven by the availability of modern computing capabilities, as well as the philosophical advantages of Bayesian thinking, applications of Bayesian data analysis have rapidly appeared in many different fields in recent years. In this section, we introduce the basic concepts of Bayesian data analysis.

2.1.1

The Bayesian Framework

The Bayesian approach to data analysis computes conditional probability distributions of unknown quantities, such as future observations, based on the observed data. Let y denote an unknown quantity of interest and D denote the observed data. The goal is to obtain a probabilistic statement about the unknown quantity y given D: p(y | D).

7

From the definition of conditional probability, we can make the following statement about the joint probability, p(y, D), which describes how y and D behave in conjunction p(y, D) = p(y)p(D | y) (2.1)

The first function on the right-hand side of the equation, p(y), is called the prior distribution of y. It is termed prior because it is specified before incorporating the observed data into the model. The form of p(y) depends on our prior knowledge about y. The second factor, p(D | y), is the likelihood function, which represents how likely the data D is, based on y. Yet, the joint probability p(y | D) is what we are really interested in ­ the distribution of the unknown quantity y. This is where Bayes' theorem comes into play.

Bayes' theorem is a result of probability theory. It forms the most fundamental basis of probability calculation. Suppose that there are two events, A and B. The axioms of probability tell us that the probability of A conditional on B is given by: p(A | B) = Likewise p(B | A) = p(B, A) p(A) (2.3) p(A, B) p(B) (2.2)

Since p(A, B) = p(B, A), rearranging (2.2) and (2.3) gives p(A | B)p(B) = p(B | A)p(A) p(A)p(B | A) p(A | B) = p(B) Equation (2.4) is the famous Bayes' theorem.

(2.4)

Following a similar process as above, we can produce the following equation from (2.1): p(y)p(D | y) = p(D)p(y | D) Rearranging it produces p(y | D) = p(y)p(D | y) , p(D) (2.6) (2.5)

which gives the desired probability distribution of the unknown quantity y conditioned on the observed data D. The denominator p(D) is the unconditional probability of D. 8

In Equation (2.6), if y is a continuous random variable, the term p(D) can be computed as p(D) = p(D, y)dy = p(y)p(D | y)dy, (2.7)

which is achieved by integrating p(D) over all possible support values of y. In the case of discrete y, the sum over y is used instead, i.e. p(D) =
y

p(y)p(D | y). p(D) is typically

called the normalising constant, or the prior predictive distribution. Its purpose is to ensure that p(y | D) integrates to one, which is required by the definition of probability density function. Because the denominator p(D) is independent of y, omitting it from Equation (2.6) yields p(y | D)  p(y)p(D | y) This states that the unnormalised posterior distribution of y is proportional to () the prior distribution times the likelihood function, i.e. Posterior  Likelihood × Prior We can summarise the preceding discussion as the following Bayesian learning process: specify a probability model that incorporates some prior knowledge about the unknown quantity, then incorporate the information from the observed data into the specified probability distribution through the likelihood function, and finally, derive (analytically or by simulation) the posterior distribution of the unknown quantity.

There are some assumptions implied in the summarised Bayesian learning process. First, the probability model specified for the unknown quantity is in parametric form, which is chosen by the individual modeler. This highlights the main difference between parametric and nonparametric modelling. Second, from the Bayesian perspective, the unknown quantity is probabilistically described and thus assumed to follow a distribution rather than have a fixed value as in the traditional 'frequentist' approach (Gill, 2002, p. 3).

2.1.2

Model Specification

Having placed ourselves in the Bayesian parametric modelling framework, the first step towards making predictive inferences about an unknown quantity is to assign a probability distribution for it. This is actually the process of encoding our prior knowledge about the unknown quantity into a probabilistic parametric model. We now discuss some of 9

the principles of assigning such a model and review the typology of prior distributions commonly applied in Bayesian work.

The Principles Developing Bayesian models requires specifying prior distributions for unknown quantities. Gill (2002, p. 114) discusses three different approaches that can be applied in the specification of prior distributions. Classical Bayesians consider prior distribution as an inconvenience and thus tend to specify a noninformative prior so as to inject the least possible amount of prior knowledge. Modern parametric Bayesians prefer conjugate priors because of the benefit of mathematical conveniences. Subjective Bayesians derive prior distributions based on existing scientific knowledge from previous empirical work in the field. In practice, these three categories are not mutually exclusive, and it is common to use a mixed approach that adopts a prior combining various aspects.

In practice, model specifications based on the observed data are much recommended. Gelman (2004, p. 14) states the following principle of how probability models can

be specified: "whenever there is replication, in the sense of many exchangeable units observed, there is scope for estimating features of a probability distribution from data and thus making the analysis more 'objective.'" Another general approach, which also specifies models based on the observed data, is summarised by Gregory (2005, p. 185). In that, constraint information is first abstracted from observed data (called testable inf ormation). Then, if there is more than one probability distribution that agrees with the given constraint information, select the one that "maximises the uncertainty in the probability distribution, while still being maximally constrained by the given testable information", so as to minimise the subjectiveness injected by the modeler.

Common Priors · Proper or Improper Prior Proper priors are distributions that add, in the case of probability mass function (PMF) for discrete variables, or integrate, in the case of probability density function (PDF) for continuous variables, to a finite quantity. The following is an example of 10

proper prior (Press, 2003, p. 54): p(y) = 1, 0y1 (2.8)

where y is an unknown quantity. This is a proper prior because it has a bounded value. It is also called a normalised proper prior, because it integrates to one. Improper priors are ones that do not possess bounded values. For example, if there are some reasons not to bound the value to be less than one, then a prior p(y) = 1, 0yk (2.9)

can be specified. This prior is unnormalised because it does not integrate to one; but it is a proper prior because it still yields a finite value.

In most circumstances, it is more common to specify proper priors because they lead to the desired proper posteriors, and thus the check of properness of the resulting posterior can be r