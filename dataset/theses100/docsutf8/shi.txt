Department of Computer Science

Hamilton, NewZealand

Best-first Decision Tree Learning

Haijian Shi

This thesis is submitted in partial fulfilment of the requirements for the degree of Master of Science at The University of Waikato.

2006 c 2007 Haijian Shi

Abstract
Decision trees are potentially powerful predictors and explicitly represent the structure of a dataset. Standard decision tree learners such as C4.5 expand nodes in depth-first order (Quinlan, 1993), while in best-first decision tree learners the "best" node is expanded first. The "best" node is the node whose split leads to maximum reduction of impurity (e.g. Gini index or information in this thesis) among all nodes available for splitting. The resulting tree will be the same when fully grown, just the order in which it is built is different. In practice, some branches of a fully-expanded tree do not truly reflect the underlying information in the domain. This problem is known as overfitting and is mainly caused by noisy data. Pruning is necessary to avoid overfitting the training data, and discards those parts that are not predictive of future data. Best-first node expansion enables us to investigate new pruning techniques by determining the number of expansions performed based on cross-validation.

This thesis first introduces the algorithm for building binary best-first decision trees for classification problems. Then, it investigates two new pruning methods that determine an appropriate tree size by combining best-first decision tree growth with cross-validation-based selection of the number of expansions that are performed. One operates in a pre-pruning fashion and the other in a post-pruning fashion. They are called best-first-based pre-pruning and best-first-based post-pruning respectively in this thesis. Both of them use the same mechanisms and thus it is possible to compare the two on an even footing. Best-first-based pre-pruning stops splitting when further splitting increases the cross-validated error, while best-first-based post-pruning takes a fully-grown decision tree and then discards expansions based on the cross-validated error. Because the two new pruning methods implement cross-validation-based

pruning, it is possible to compare the two to another cross-validation-based pruning method: minimal cost-complexity pruning (Breiman et al., 1984). The two main results are that best-first-based pre-pruning is competitive with best-first-based post-pruning if the so-called "one standard error rule" is used. However, minimal i

cost-complexity pruning is preferable to both methods because it generates smaller trees with similar accuracy.

ii

Acknowledgments
This thesis would not have been possible without the support and assistance of the people in the Department of Computer Science at the University of Waikato, especially the members of machine learning group. These people provides me a very good environment to do my research in New Zealand.

First and foremost, I would like to thank my supervisor Dr.

Eibe Frank for

guiding me through the whole thesis. He provided a lot of useful materials and helped with implementation problems I encountered during the development. He also helped me revise over the thesis draft. Moreover, he gave me valuable suggestions of how to run the experiments presented in this thesis.

Special thanks to Dr.

Mark Hall who told me how to use the rpart package

(the implementation of the CART system in the language R). This helped me do a comparison between my basic implementation of the CART system in Java and the rpart package in order to make sure that there is no problem in my implementation.

I would also like to thank all members in the machine learning group. I gained so much useful knowledge from the weekly meetings and e-mails sent to me. These guys are so friendly and helped me solve many problems I met.

iii

Contents
Abstract Acknowledgments List of Figures List of Tables 1 Introduction 1.1 1.2 Basic concepts of machine learning . . . . . . . . . . . . . . . . . . . . Decision trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2.1 1.2.2 1.3 1.4 1.5 Standard decision trees . . . . . . . . . . . . . . . . . . . . . . Best-first decision trees . . . . . . . . . . . . . . . . . . . . . . i iii ix xi 1 1 2 3 3 5 6 7 9 9 10 16 18 19 20 21 23 25 27 29 30 32

Pruning in decision trees . . . . . . . . . . . . . . . . . . . . . . . . . . Motivation and objectives . . . . . . . . . . . . . . . . . . . . . . . . . Thesis structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Background 2.1 Pruning methods related to cross-validation . . . . . . . . . . . . . . . 2.1.1 2.1.2 2.2 Minimal cost-complexity pruning . . . . . . . . . . . . . . . . . Other pruning methods related to cross-validation . . . . . . .

Pruning in standard decision trees . . . . . . . . . . . . . . . . . . . . 2.2.1 2.2.2 Pre-pruning and post-pruning . . . . . . . . . . . . . . . . . . . Comparisons of pre-pruning and post-pruning . . . . . . . . . .

2.3 2.4

Best-first decision trees in boosting . . . . . . . . . . . . . . . . . . . . Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Best-first decision tree learning 3.1 Splitting criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.1 3.1.2 3.2 Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Gini index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Splitting rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v

3.2.1 3.2.2 3.2.3 3.3

Numeric attributes . . . . . . . . . . . . . . . . . . . . . . . . . Nominal attributes . . . . . . . . . . . . . . . . . . . . . . . . . The selection of attributes . . . . . . . . . . . . . . . . . . . . .

33 35 39 41 42 44 45 46 48 49 51 52 55 55 58 62 66

Best-first decision trees . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 3.3.2 The best-first decision tree learning algorithm . . . . . . . . . . Missing values . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.4

Pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.1 3.4.2 3.4.3 Best-first-based pre-pruning . . . . . . . . . . . . . . . . . . . . Best-first-based post-pruning . . . . . . . . . . . . . . . . . . . The 1SE rule in best-first-based pre-pruning and post-pruning

3.5 3.6

Complexity of best-first decision tree induction . . . . . . . . . . . . . Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Experiments 4.1 4.2 4.3 4.4 4.5 Datasets and methodology . . . . . . . . . . . . . . . . . . . . . . . . . Gini index versus information . . . . . . . . . . . . . . . . . . . . . . . Error rate versus root mean squared error . . . . . . . . . . . . . . . . Heuristic search versus exhaustive search . . . . . . . . . . . . . . . . . The effects of the 1 SE rule in best-first-based pre-pruning and postpruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.1 4.5.2 4.5.3 4.6 Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Tree size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

68 69 69 72 72 72 74 74 76

Comparing best-first-based pre-pruning and post-pruning . . . . . . . 4.6.1 4.6.2 4.6.3 4.6.4 Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Tree size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training time . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.7

Comparing best-first-based pre-pruning and post-pruning to minimal cost-complexity pruning . . . . . . . . . . . . . . . . . . . . . . . . . . 4.7.1 4.7.2 4.7.3 4.7.4 Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Tree size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training time . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi 78 78 79 82 82

4.8

The effects of training set size on tree complexity for best-first-based pre-pruning and post-pruning . . . . . . . . . . . . . . . . . . . . . . . 84 87 91 91 93 95 95

4.9

Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Conclusions 5.1 5.2 Summary and conclusions . . . . . . . . . . . . . . . . . . . . . . . . . Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Appendices A Possible binary splits on the attribute temperature B Accuracy and tree size of pre-pruning and post-pruning for different training set sizes

97

vii

List of Figures
1.1 Decision trees: (a) a hypothetical depth-first decision tree, (b) a hypothetical best-first decision tree. . . . . . . . . . . . . . . . . . . . . . . 2.1 2.2 3.1 3.2 4

(a) The tree T , (b) a branch of T Tt5 , (c) the pruned subtree T  = T -Tt5 . 10 The tree for the parity problem. . . . . . . . . . . . . . . . . . . . . . 21 26

The best-first decision tree on the iris dataset. . . . . . . . . . . . . . (a) The fully-expanded best-first decision tree; (b) the fully-expanded standard decision tree; (c) the best-first decision tree with three expansions from (a); (d) the standard decision tree with three expansions from (b). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

27 30 31 34 36 43

3.3 3.4 3.5 3.6 3.7 3.8

Possible splits for the four attributes of the weather dataset.

. . . . .

The information value and the Gini index in a two-class problem. . . . Possible binary splits on the attribute temperature. . . . . . . . . . . . Possible binary splits on the attribute outlook. . . . . . . . . . . . . . . The best-first decision tree learning algorithm. . . . . . . . . . . . . . The accuracy for each number of expansions of best-first decision tree learning on the iris dataset. . . . . . . . . . . . . . . . . . . . . . . . .

45 47 49 96

3.9

The best-first-based pre-pruning algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.10 The best-first-based post-pruning algorithm.

A.1 Possible binary splits on the attribute temperature. . . . . . . . . . . .

ix

List of Tables
2.1 The sequence of pruned subtrees from T1 , accompanied by their corresponding  values on the balance-scale dataset. . . . . . . . . . . . . . 2.2 2.3 2.4 3.1 3.2 3.3 Rcv (Tk ) for different seeds without 1SE on the balance-scale dataset. . Rcv (Tk ) for different seeds with 1SE on the balance-scale dataset. . . . A parity problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 14 15 20 28 38

The instances of the weather dataset. . . . . . . . . . . . . . . . . . . . The class frequencies for the artificial attribute A. . . . . . . . . . . . Splitting subsets and their corresponding Gini gains and information gains on the artificial attribute A. . . . . . . . . . . . . . . . . . . . .

40

3.4

The error rate and the root mean squared error for each expansion using best-first decision tree learning on the glass dataset. . . . . . . . . . . 48

3.5

The error rate and the root mean squared error for each expansion with 1SE using best-first decision tree learning on the glass dataset. . . . . 50 57

4.1 4.2

The 38 UCI datasets and their properties. . . . . . . . . . . . . . . . . The accuracy of best-first-based post-pruning using (a) the Gini index and (b) the information. . . . . . . . . . . . . . . . . . . . . . . . . . .

60

4.3

The tree size of best-first-based post-pruning using (a) the Gini index and (b) the information. . . . . . . . . . . . . . . . . . . . . . . . . . . 61

4.4

The accuracy of best-first-based pre-pruning and post-pruning using (a) the error rate and (b) the root mean squared error. . . . . . . . . . 64

4.5

The tree size of best-first-based pre-pruning and post-pruning using (a) the error rate and (b) the root mean squared error. . . . . . . . . . . . 65

4.6

The multi-class datasets used for comparing heuristic search and exhaustive search. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 68

4.7 4.8

The accuracy for heuristic search and exhaustive search.

. . . . . . .

The accuracy for heuristic search and exhaustive search on three datasets whose attribute with maximum number of values has been deleted. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi 68

4.9

The accuracy of best-first-based pre-pruning and post-pruning both with and without the 1SE rule. . . . . . . . . . . . . . . . . . . . . . . 70

4.10 The tree size of best-first-based pre-pruning and post-pruning both with and without the 1SE rule. . . . . . . . . . . . . . . . . . . . . . . . . . 4.11 Comparing the accuracy of best-first-based pre-pruning and postpruning (both with and without the 1SE rule). . . . . . . . . . . . . . 4.12 Comparing the tree size of best-first-based pre-pruning and postpruning (both with and without the 1SE rule). . . . . . . . . . . . . . 4.13 Comparing the training time of best-first-based pre-pruning and postpruning (both with and without the 1SE rule). . . . . . . . . . . . . . 4.14 Comparing the accuracy of the two new pruning methods and minimal cost-complexity pruning (both with and without the 1SE rule). . . . . 4.15 Comparing the tree size of the two new pruning methods and minimal cost-complexity pruning (both with and without the 1SE rule). . . . . 4.16 Comparing the training time of the two new pruning methods and minimal cost-complexity pruning (both with and without the 1SE rule). . 4.17 The accuracy and tree size of best-first-based pre-pruning on different training set sizes (both with and without the new 1SE rule). . . . . . . 4.18 The accuracy and tree size of best-first-based post-pruning for different training set sizes (both with and without the new 1SE rule). . . . . . . 4.19 The effect of random data reduction on tree complexity for best-firstbased pre-pruning (both with and without the new 1SE rule). . . . . . 4.20 The effect of random data reduction on tree complexity for best-firstbased post-pruning (both with and without the new 1SE rule). . . . . B.1 The accuracy of best-first-based pre-pruning without the new 1SE rule for different training set sizes. . . . . . . . . . . . . . . . . . . . . . . . B.2 The tree size of best-first-based pre-pruning without the new 1SE rule for different training set sizes. . . . . . . . . . . . . . . . . . . . . . . . B.3 The accuracy of best-first-based pre-pruning with the new 1SE rule for different training set sizes. . . . . . . . . . . . . . . . . . . . . . . . . . B.4 The tree size of best-first-based pre-pruning with the new 1SE rule for different training set sizes. . . . . . . . . . . . . . . . . . . . . . . . . . B.5 The accuracy of best-first-based post-pruning without the 1SE rule on for different training set sizes. . . . . . . . . . . . . . . . . . . . . . . . xii 99 99 98 98 97 87 86 85 85 83 81 80 77 75 73 71

B.6 The tree size of best-first-based post-pruning without the 1SE rule for different training set sizes. . . . . . . . . . . . . . . . . . . . . . . . . . 100 B.7 The accuracy of best-first-based post-pruning with the 1SE rule for different training set sizes. . . . . . . . . . . . . . . . . . . . . . . . . . 100 B.8 The tree size of best-first-based post-pruning with the 1SE rule for different training set sizes. . . . . . . . . . . . . . . . . . . . . . . . . . 101

xiii

Chapter 1 Introduction
In the early 1990's, the establishment of the Internet made large quantities of data to be stored electronically, which was a great innovation for information technology. However, the question is what to do with all this data. Data mining is the process of discovering useful information (i.e. patterns) underlying the data. Powerful

techniques are needed to extract patterns from large data because traditional statistical tools are not efficient enough any more. Machine learning algorithms are a set of these techniques that use computer programs to automatically extract models representing patterns from data and then evaluate those models. This thesis investigates a machine learning technique called best-first decision tree learner. It evaluates the applicability of best-first decision tree learning on real-world data and compares it to standard decision tree learning.

The remainder of this chapter is structured as follows.

Section 1.1 describes

some basic concepts of machine learning which are helpful for understanding bestfirst decision tree learning. Section 1.2 discusses the basic ideas underlying best-first decision trees and compare them to standard decision trees. The pruning methods for best-first decision trees are presented briefly in Section 1.3. The motivation and objectives of this thesis are discussed in Section 1.4. Section 1.5 lists the structure of the rest of this thesis.

1.1

Basic concepts of machine learning

To achieve the goals of machine learning described above, we need to organise the input and output first. The input consists of the data used to build models. The input involves concepts, instances, attributes and datasets. The thing which is to be learnt is called the concept. According to Witten and Frank (2005), there are four different styles of concepts in machine learning, classification learning, association learning, 1

clustering and regression learning. Classification learning takes a set of classified examples (e.g. examples with class values) and uses them to build classification models. Then, it applies those models to classifying unseen examples. Association learning takes account of any association between features not just predicting class values. Clustering groups a set of similar examples together according to some

criteria. Regression learning uses a numeric value instead of a class label for the class of each example.

The input of machine learning consists of a set of instances (e.g.

rows, exam-

ples or sometimes observations). Each instance is described by a fixed number of attributes (i.e. columns), which are assumed to be either nominal or numeric, and a label which is called class (when the task is a classification task). The set of instances is called a dataset. The output of machine learning is a concept description. Popular types of concept descriptions are decision tables, decision trees, association rules, decision rules, regression trees and instance-based representations (Witten & Frank, 2005).

The four concepts described above can be organised into two categories, supervised learning and unsupervised learning. Classification learning and regression

learning are supervised learning algorithms. In supervised classification learning, the induction algorithm first makes a model for a given set of labelled instances. Then, the algorithm applies the model to unclassified instances to make class predictions. In supervised regression learning, the induction algorithm maps each instance to a numeric value, not a class label. Association learning and clustering are unsupervised learning tasks that deal with discovering patterns for unlabelled instances. The

best-first decision tree learner investigated in this thesis is a learning algorithm for supervised classification learning.

1.2

Decision trees

A decision tree is a tree in which each internal node represents a choice between a number of alternatives, and each terminal node is marked by a classification. Decision trees are potentially powerful predictors and provide an explicit concept description for a dataset. In practice, decision tree learning is one of the most popular technique 2

in classification because it is fast and produces models with reasonable performance. In the context of this thesis there are two sorts of decision trees. One is constructed in depth-first order and called "standard" decision tree. The other is constructed in best-first order and called "best-first" decision tree. The latter type is the focus of this thesis.

1.2.1

Standard decision trees

Standard algorithms such as C4.5 (Quinlan, 1993) and CART (Breiman et al., 1984) for the top-down induction of decision trees expand nodes in depth-first order in each step using the divide-and-conquer strategy. Normally, at each node of a decision tree, testing only involves a single attribute and the attribute value is compared to a constant. The basic idea of standard decision trees is that, first, select an attribute to place at the root node and make some branches for this attribute based on some criteria (e.g. information or Gini index). Then, split training instances into subsets, one for each branch extending from the root node. The number of subsets is the same as the number of branches. Then, this step is repeated for a chosen branch, using only those instances that actually reach it. A fixed order is used to expand nodes (normally, left to right). If at any time all instances at a node have the same class label, which is known as a pure node, splitting stops and the node is made into a terminal node. This construction process continues until all nodes are pure. It is then followed by a pruning process to reduce overfittings (see Section 1.3).

1.2.2

Best-first decision trees

Another possibility, which so far appears to only have been evaluated in the context of boosting algorithms (Friedman et al., 2000), is to expand nodes in best-first order instead of a fixed order. This method adds the "best" split node to the tree in each step. The "best" node is the node that maximally reduces impurity among all nodes available for splitting (i.e. not labelled as terminal nodes). Although this results in the same fully-grown tree as standard depth-first expansion, it enables us to investigate new tree pruning methods that use cross-validation to select the number of expansions. Both pre-pruning and post-pruning can be performed in this way, which enables a fair comparison between them (see Section 1.3). 3

a.

N1 N2 Leaf N3 N4 Leaf Leaf

b.

N1 N3 Leaf N4 N2 Leaf Leaf

Leaf Leaf

Leaf Leaf

Figure 1.1: Decision trees: (a) a hypothetical depth-first decision tree, (b) a hypothetical best-first decision tree.

Best-first decision trees are constructed in a divide-and-conquer fashion similar to standard depth-first decision trees. The basic idea of how a best-first tree is built is as follows. First, select an attribute to place at the root node and make some branches for this attribute based on some criteria. Then, split training instances into subsets, one for each branch extending from the root node. In this thesis only binary decision trees are considered and thus the number of branches is exactly two. Then, this step is repeated for a chosen branch, using only those instances that actually reach it. In each step we choose the "best" subset among all subsets that are available for expansions. This constructing process continues until all nodes are pure or a specific number of expansions is reached. Figure 1.1 shows the difference in split order between a hypothetical binary best-first tree and a hypothetical binary depth-first tree. Note that other orderings may be chosen for the best-first tree while the order is always the same in the depth-first case.

The problem in growing best-first decision trees is now how to determine which attribute to split on and how to split the data. Because the most important objective of decision trees is to seek accurate and small models, we try to find pure nodes as soon as possible. To measure purity, we can use its opposite, impurity. There are many criteria to measure node impurity. For example, the CART system (Breiman et al., 1984) uses Gini index and C4.5 (Quinlan, 1993) uses information. In this thesis, both the information and the Gini index are used to compare their different performance. The goal is to aim an attribute to split on that can maximally reduce impurity.

Split selection methods used in this thesis are as follows.

For a numeric at-

tribute, the split is the same as that for most decision trees such as the CART system 4

(Breiman et al., 1984): the best split point is found and split is performed in a binary fashion. For example, a split on the iris dataset might be petallength < 3.5 and petallength  3.5. For a nominal attribute, the node is also split into exactly two branches no matter how many values the splitting attribute has. This is also the same as the one used in the CART system. In this case, the objective is to find a set of attribute values which can maximally reduce impurity. Split methods are discussed in detail later in Chapter 3. For binary trees as the ones used in this thesis, it is obvious that each split step increases the number of terminal nodes by only one.

The information and the Gini gain are also used to determine node order when expanding nodes in the best-first tree. The best-first method always chooses the node for expansion whose corresponding best split provides the best information gain or Gini gain among all unexpanded nodes in the tree.

1.3

Pruning in decision trees

Fully-expanded trees are sometimes not as good as smaller trees because of noise and variability in the data, which can result in overfitting. To prevent the problem and build a tree of the right size, a pruning process is necessary. In practice,

almost all decision tree learners are accompanied by pruning algorithms. Generally speaking, there are two kinds of pruning methods, one that performs pre-pruning and another one that performs post-pruning. Pre-pruning involves trying to decide to stop splitting branches early when further expansion is not necessary. Post-pruning constructs a complete tree first and prunes it back afterwards based on some criteria. Pre-pruning seems attractive because it avoids expanding the full tree and throwing some branches away afterwards, which can save computation time, but post-pruning is considered preferable because of "early stopping": a significant effect may not be visible in the tree grown so far and pre-pruning may stop too early. However, empirical comparisons are rare.

Because all pruning procedures evaluated in this thesis are based on crossvalidation, we now briefly explain how it works. In cross-validation, a fixed number of folds is decided first. Assuming the number is ten, the data is separated into ten approximately equal partitions and each in turn is used for testing and the remainder 5

is used for training. Thus, every instance is used for testing exactly once. This is called ten-fold cross-validation. An error estimate is calculated on the test set for each fold and the ten resulting error estimates are averaged to get the overall error estimate. When pruning decision trees based on the methods investigated in this thesis, the final tree size is decided according to this average error estimate.

In the case of best-first decision trees, pre-pruning and post-pruning can be easily performed by selecting the number of expansions based on the error estimate by cross-validation, and this is what makes them different from standard decision trees. Here is the basic idea of how they work. For both pruning methods, the trees in all folds are constructed in parallel (e.g. ten trees for a ten-fold cross-validation). For each number of expansion, the average error estimate is calculated based on the temporary trees in all folds. Pre-punning simply stops growing the trees when further splitting increases the average error estimate and chooses the previous number of expansion as the final number of expansions. Post-pruning continues expanding nodes until all the trees are fully expanded. Then it chooses the number of expansions whose average error estimate is minimal. In both cases the final tree is then built based on all the data and the chosen number of expansions.

This thesis compares the two new pruning methods to another cross-validationbased pruning method as implemented in the CART system, called "minimal cost-complexity pruning" (Breiman et al., 1984). This pruning method is another kind of post-pruning technique. The basic idea of this pruning method is that it tries to first prune those branches that relative to their size which leads to the smallest increase in error on the training data. The details of minimal cost-complexity

pruning are described in Chapter 2. The pre-pruning and post-pruning algorithms for best-first decision trees are described in Chapter 3.

1.4

Motivation and objectives

Significant research efforts have been invested into depth-first decision tree learners. Best-first decision tree learning has so far only been applied as the base learner in boosting (Friedman et al., 2000), and has not been paid too much attention in spite of the fact that it makes it possible to implement pre-pruning and post-pruning in 6

a very simple and elegant fashion, based on using the number of expansions as a parameter. Thus, this thesis investigates best-first decision tree learning. It investigates the effectiveness of pre-pruning and post-pruning using best-first decision trees. Best-first-based pre-pruning and post-pruning can be compared on an even footing as both of them are based on cross-validation. Minimal cost-complexity pruning is used as the benchmark pruning technique as it is also based on cross-validation and known to perform very well. As mentioned before, the best-first decision trees investigated in this thesis are binary, and the searching time of finding the optimum split for a nominal attribute using exhaustive search is exponential in the number of attribute values of this attribute, so we need to seek an efficient splitting method for nominal attributes. This thesis also lists two efficient search methods for finding binary splits on nominal attributes. One is for two-class problems (Breiman et al., 1984) and the other (i.e. heuristic search) is for multi-class problems (Coppersmith et al., 1999).

In order to fulfil the tasks presented above, the objectives of this thesis are as follows: 1. To evaluate the applicability of best-first decision tree learning to real-world data. 2. To compare best-first-based pre-pruning and post-punning on an even footing using the best-first methodology. 3. To compare the two new best-first-based pruning algorithms to minimal costcomplexity pruning. 4. To compare heuristic search and exhaustive search for binary splits on nominal attributes in multi-class problems.

1.5

Thesis structure

To achieve the objectives described above, the rest of this thesis is organised in four chapters.

The background for the work presented in this thesis is described in Chapter 2. Some known pruning methods, related to cross-validation, are discussed first. 7

Minimal cost-complexity pruning is discussed in detail as it is involved in the experiments presented in this thesis. Other pruning methods are briefly described. Then, the principles of pre-pruning and post-pruning for standard decision trees are explained and compared. Finally, the paper that introduced best-first decision trees (Friedman et al., 2000), which applied best-first decision trees to boosting, is briefly described.

Chapter 3 discusses the best-first decision trees used in thesis in detail.

It

presents impurity criteria, splitting methods for attributes, error estimates to determine the number of expansions and the algorithm for constructing best-first decision trees. Pre-pruning and post-pruning algorithms for best-first decision trees are also described in this chapter. We discuss how the one standard error rule (i.e. the 1SE rule) can be used in these two pruning methods.

Chapter 4 provides the experimental results for 38 standard benchmark datasets from the UCI repository (Blake et al., 1998) for best-first decision tree learning presented in this thesis. We evaluate best-first-based pre-pruning and post-pruning using different splitting criteria and different error estimates (determine the number of expansions) to find which splitting criterion and error estimate is better. The performance of the two new pruning methods is compared in terms of classification accuracy, tree size and training time. Then, we compare their performance to minimal cost-complexity pruning in the same way. Experiments are also evaluated to see whether tree size obtained by best-first-based pre-pruning and post-pruning is influenced by training set size. Results for heuristic search and exhaustive search for finding binary splits on nominal attributes are also discussed.

Chapter 5 summarises material presented in this thesis, draws conclusions from th