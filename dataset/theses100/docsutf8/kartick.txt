3D Shape Reconstruction from Multiple Range Image Views

A thesis submitted in partial fulfillment of the requirements for the degree of

Master of Science
in Physics and Electronic Engineering

at The University of Waikato by Kartick Ganapathi Annadurai

Hamilton 2006

ii

i

Dedicated To My Mum and Dad My Brother Shathiyakkumar My Sister Indumathi

ii

iii

Acknowledgement
I am extremely grateful and privileged to be educated in Image Processing by a distinguished physicist and chief supervisor Dr Michael Cree. Personal gratitude for his patience, guidance and immense support over the last two years as I have worked towards this thesis.

In addition, without the help and support of many people it may not have been possible to complete this thesis. To Dr Adrian Dorrington I am more grateful, for his efforts during my work on simulated study. Also thank you Mr Richard Convoy for your help and support during acquisition with the Waikato Range Imager. Thanks to staff Heidi and Bruce in the Department of Physics & Electronic Engineering for their timely assistance. Thanks to flat mate Mr Anandajothi and Mr Dipu for taking extreme care of my health and helping with thesis writing.

Many thanks must go to friends Kel, Jono, Inky, AJ, Dipu and Bobby for their valuable time. Special thanks to Shene Kurien for his timely advice and encouragement during my thesis. Above all, very special thanks to Mum and Dad for their patience and support.

iv

v

Abstract
Shape reconstruction of different three dimensional objects using multiple range images has evolved recently within the recent past. In this research shape reconstruction of a three dimensional object using multiple range image views is investigated. Range images were captured using the Waikato Range Imager. This range images camera is novel in that it uses heterodyne imaging and is capable of acquiring range images with precision less than a millimeter simultaneously over a full field. Multiple views of small objects were taken and the FastRBF was explored as a mean of registration and surface rendering. For comparison to the real range data, simulated range data under noise free condition were also generated and reconstructed with the FastRBF tool box. The registration and reconstruction of simple object was performed using different views with the FastRBF toolbox. Analysis of the registration process showed that the translation error produced due to distortion during registration of different views hinders the process of reconstructing a complete surface. While analyzing the shape reconstruction using the FastRBF tool it is also determined that a small change in accuracy values can affect the interpolation drastically. Results of reconstruction of a real 3D object from multiple views are shown.

Table of contents
Acknowledgement----------------------------------------------------iii Abstract -----------------------------------------------------------------v Table of contents ---------------------------------------------------- vii List of figures ---------------------------------------------------------xi List of Tables--------------------------------------------------------- xv 1 Introduction --------------------------------------------------------1 2 Background Theory-----------------------------------------------5
2.1
2.1.1 2.1.2 2.1.3 2.1.4 2.1.5

Digital image processing --------------------------------------------- 5
Image Acquisition ------------------------------------------------------------ 6 Image Enhancement ---------------------------------------------------------- 6 Image Restoration ------------------------------------------------------------ 7 Image Analysis---------------------------------------------------------------- 7 Image Synthesis--------------------------------------------------------------- 8

2.2 2.3 2.4

Image data handling--------------------------------------------------- 9 Range imaging -------------------------------------------------------- 10 Geometrical Representation----------------------------------------- 11

viii 2.4.1 Euclidean coordinate ------------------------------------------------------- 11

2.5
2.5.1 2.5.2

Projections------------------------------------------------------------- 12
Parallel projections --------------------------------------------------------- 12 Central projections---------------------------------------------------------- 13

2.6 2.7
2.7.1 2.7.2

Coordinate Transformation ----------------------------------------- 14 Distortions------------------------------------------------------------- 16
Radial Distortion------------------------------------------------------------ 16 Geometrical Distortion ----------------------------------------------------- 17

2.8 2.9 2.10

Background Segmentation ------------------------------------------ 18 3D Shape Reconstruction Methods Reviewed-------------------- 19 Overview of Registration Process ------------------------------- 21

3 Equipment and hardware ---------------------------------------25
3.1
3.1.1 3.1.2 3.1.3 3.1.4 3.1.5 3.1.6 3.1.7

Image Capturing ------------------------------------------------------ 25
Imaging Lidar --------------------------------------------------------------- 25 Image Intensifier ------------------------------------------------------------ 27 Signal Generator ------------------------------------------------------------ 28 Ranger Camera-------------------------------------------------------------- 29 CCD image Sensor --------------------------------------------------------- 29 Light Source ----------------------------------------------------------------- 30 Grabber----------------------------------------------------------------------- 31

3.2

Ranger Precision------------------------------------------------------ 31

4 Experimental Methodologies ----------------------------------33
4.1
4.1.1 4.1.2

FastRBF tool ---------------------------------------------------------- 33
Fitting RBF to surface data ------------------------------------------------ 35 Mesh Generation------------------------------------------------------------ 36

ix 4.1.3 Using FastRBF -------------------------------------------------------------- 38

4.2
4.2.1 4.2.2 4.2.3 4.2.4

Simulation Work ----------------------------------------------------- 39
Principle---------------------------------------------------------------------- 39 Setup Description ----------------------------------------------------------- 40 Procedure -------------------------------------------------------------------- 41 Setup Values used for the simulation study ----------------------------- 47

4.3 4.4

Experiment using Real Range Data -------------------------------- 48 Registration and Reconstruction ----------------------------------- 57

5 Results ------------------------------------------------------------ 59
5.1 5.2 Simulated Reconstruction results ---------------------------------- 59 Experiment Reconstruction results--------------------------------- 65

6 Conclusion ------------------------------------------------------- 79 References ------------------------------------------------------------ 83

List of figures
Figure 2.1 : Block diagram showing the stages in Digital Image Processing............... 5 Figure 2.2 : A surface point S in Euclidean coordinate system .................................. 12 Figure 2.3 : Image captured using parallel projection ................................................ 13 Figure 2.4 : Scene point captured on image plane using central projection ............... 14 Figure 2.5 : The three three-dimensional rotations. .................................................... 15 Figure 2.6 : Distortion produced on a checked plane while image acquiring. (Source: www.umich.edu/~lowbrows/guide/opticaljargon.html) ............................................. 16 Figure 2.7 : Geometrical distortion resulting from increased path length of off axis objects. ........................................................................................................................ 18 Figure 3.1 : Heterodyne imaging lidar systems (Source: Payne et al., 2006)............. 26 Figure 3.2 : Image intensifier depicting how a single photon is multiplied to many due to electron acceleration (Source: Payne et al., 2006) ........................................... 28 Figure 3.3 : CCD used in Ranger camera ................................................................... 30 Figure 4.1 : Plot showing Fitter accuracy and evaluation accuracy. (Source : Farfield technology).................................................................................................................. 34 Figure 4.2 : Creating density data from surface normals (Source: Farfield technology) ..................................................................................................................................... 36 Figure 4.3 : Comparison of RBF evaluations (Source : Farfield technology) ............ 37 Figure 4.4 : The results for different mesh optimisation methods (Source: Farfield technology).................................................................................................................. 38 Figure 4.5 : Scene used for simulation work .............................................................. 39 Figure 4.6: Simulation setup showing image plane, centre of projection and the scene. ..................................................................................................................................... 41 Figure 4.7 : Ray tracing - Intersection point of the ray originating at R0 with directional vector Rd on the finite plane at R(t)........................................................... 43 Figure 4.8 : A ray intersecting at two points R and R1. The shortest range value for the is recorded. ............................................................................................................ 45 Figure 4.9 : Simulated range image ............................................................................ 46

xii Figure 4.10 : Experimental setup used for capturing the test object. ..........................48 Figure 4.11 : Test object bear used in this study as scene is placed on a turntable.....50 Figure 4.12 : Range image front view of bear captured from ranger camera..............51 Figure 4.13 : Range image cropped to 351 × 291 from 512 × 512 .............................52 Figure 4.14 : Establishing world coordinates for every pixel in the range image.......53 Figure 4.15 : Range image showing 0 o overlapped with 180 o (dashed line) and the x coordinate centre .........................................................................................................55 Figure 4.16 : Range image showing calculation of z axis centre (see explanation for detail) ...........................................................................................................................56 Figure 5.1 : Simulated 0o Range image for 101×101 resolution image ......................59 Figure 5.2 : Simulated 270o Range image for 101×101 resolution image ..................60 Figure 5.3 : Simulated 0o Range image for 512×512 resolution image ......................60 Figure 5.4 : Simulated 270o Range image for 512×512 resolution image ..................61 Figure 5.5 : Reconstructed surface using 0o, 30o, 180o and 330o range data (front on) .....................................................................................................................................63 Figure 5.6 : Reconstructed surface using 0o, 30o, 180o and 330o range data (side view) .....................................................................................................................................63 Figure 5.7 : Reconstructed surface using 0o, 30o, 180o and 330o range data (side view) .....................................................................................................................................64 Figure 5.8: Reconstructed surface using 0o, 30o, 180o and 330o range data (rear view) .....................................................................................................................................64 Figure 5.9 : a) Intensity image and b) range image for the 0 o view............................65 Figure 5.10 : a) Intensity image and b) range image for 30 o view..............................66 Figure 5.11 : a) Intensity image and b) range image for 270 o view............................66 Figure 5.12 : Segmentation of bear based on range values only .................................67 Figure 5.13 : Segmentation based on using both range and intensity values ..............68 Figure 5.14 : Shape reconstructed 0o degree range image...........................................69 Figure 5.15 : Shape reconstruction using Fast RBF for two set of data (0o and 30o)..70 Figure 5.16 : Scatter plot showing two set of range data 0o and rotated 30o ..............72 Figure 5.17 : Error between two data sets determined using point evaluation............72

xiii Figure 5.18 : Deviation error between two data sets across the image at five locations ..................................................................................................................................... 73 Figure 5.19 : Reconstructed bear with corrected range data....................................... 74 Figure 5.20 : Side view of the reconstructed bear....................................................... 75 Figure 5.21 : Side view of the reconstructed bear showing the rotated 30o range data ..................................................................................................................................... 75 Figure 5.22 : Scatter plot showing two set of range data (0o and new rotated 30o ) ... 76 Figure 5.23 : Error between two new data sets determined using point evaluation.... 76 Figure 5.24 : Deviation error between two data sets across the image at five locations for the coordinate registration values.......................................................................... 77 Figure 5.25 : Linear fit for a single row across the deviation error image.................. 77

xiv

List of Tables
Table 1 : Scene coordinate values used in the simulated robot scene......................... 40 Table 2 : Showing the values used for simulation ...................................................... 47 Table 3 : Experimental Setup values........................................................................... 49 Table 4: Showing the number pixels and corresponding measurement across the range image and the bear ...................................................................................................... 54

xvi

1

1 Introduction
Methods to reconstruct the shapes of different three dimensional objects have evolved rapidly in recent years. The speed and accuracy of digitizing technologies have given much to advance in the areas of physics and electrical engineering, including the development of lasers and high speed sampling and timing circuitry. Such technologies allow us to take detailed shape measurements with precision better than 1 part per 1000 at rates exceeding 10000 samples per second. To capture the complete shape of an object, many thousands of samples must be acquired. The resulting mass of data requires algorithms that can efficiently and reliably generate models from these samples. The images used in this study are range images in which the distance from the camera to a point on the scene is encoded in the image as a grey value. Each range image provides a detailed description of an object as seen from one point of view. Before attempting to reconstruct the entire shape of an object, we require multiple range images. A single range image is taken from one point of view and much of the object being viewed may be obscured, therefore multiple range images are taken. Some objects are too complex to be captured in detail by a small number of range images. For example, faces lying in mutually orthogonal directions and their opposites sides of a six faced cube. Due to self occlusions, an object may require a large number of range scans to see every visible point. Thus multiple range images are required for the reconstruction of the surface. The principle goal of this thesis is to explore, merging multiple range images taken from different views of an object, to reconstruct the full 3D surface. In this study a simulated reconstruction and real world reconstruction are undertaken. The simulated reconstruction study is carried out to know how well the reconstruction algorithm works in a noise free situation and under known conditions. The simulated object which resembles a roboshape is generated by computer (see Figure 4.5). The simulated object is rotated and a number of range images are recorded from different views.

2 The real test object includes a bear (see Figure 4.11). The test object is of uniform colouration. However the presence of little discolouration were permitted. The test object is manually rotated on a turntable, and a number of range images are recorded from different views, as the object is rotated full circle. The three dimensional data for the scene is obtained from the range images. Shape reconstruction used in this study is the generation of surface data and does not include the generation of interior data as in the case of computer tomography, nor does it include further information on colour. The objects or scene to be imaged are also assumed to be of a static nature, as it does take some minutes to acquire the range data. The acquiring of range image is performed using the Waikato ranger system. The main advantage of using Waikato ranger system is its high precision in range values. After capturing, in order to merge a set of range images into a single description of an object, it is necessary to place them all in the same coordinate system; that is they must be registered or aligned with respect to each other. The alignment arises from prior knowledge of the pose of the rangefinder when acquiring the range image. After alignment, the range images provide the starting point for performing a surface reconstruction. The proposed work uses FastRBF toolbox for interpolation and reconstruction. This tool takes the aligned range data as input; it interpolates them and constructs it in to a complete surface with given resolution, which is the final goal of this research. Exploring the FastRBF tool for reconstruction using the Waikato ranger data forms one of the goals of this research. There are only few models available that uses FastRBF tool for reconstruction using range images. Also there have been no shape reconstruction model yet published that uses FastRBF software for the Waikato ranger data. So the detailed study and results of my proposed work forms a platform for those uses the Waikato ranger data for reconstruction. The thesis is structured in the following manner. First in chapter two we introduce background theory which forms the fundamental basis of the research. In this chapter image processing techniques that play a large role in the processing of the visual data are detailed as; fundamental theorems, definitions about the representation and acquisition of the scene. Finally an overview of the registration process and

3 review of the available reconstruction models is presented to finish up the background theory. The chapter three the equipment and hardware used in the study is described. Following that in chapter four the testing methodology and the proposed work on this study is discussed in the experimental methodology chapter. The chapter begins with discussion on the FastRBF software, the tool used for interpolating the data points to represent a surface. Then discussion about the experimental setup and process of performing shape reconstruction for both simulated study and the test objects is explained. This chapter contains some original content introduced by myself such as exploring the FastRBF software for reconstruction of surface using the range data obtained from the Waikato ranger system. The results of which were discussed in the following chapter. In chapter five results for the reconstructed outputs of the simulated work and test object are presented. A discussion detailing the parameters at which particular results were obtained as well as features or characteristics worthy of attention are also identified. Some errors identified are rectified and new results with rectified errors were discussed. In the final chapter the overall conclusion of the research is given, and the proposal for future work on the research topic covered.

4

5

2 Background Theory
This chapter discusses the basics of the digital image processing and the stages involved in that, following an overview of range imaging, geometrical representation of the surface and major distortions affecting the acquisition process. Finally, an overview of models available for shape reconstruction using multiple range images is discussed.

2.1 Digital image processing
Digital image processing is the processing of digital images by means of digital computation (Gonzalez and Woods, 2002). Digital images are normally expressed as an array of numbers where the elements of the array are called pixels. Normally the value of each pixel represents the intensity of light received however in this thesis we shall often use images in which the value of each pixel is range between the scene and camera. The five major process involved in digital image processing are image acquisition, image enhancement, image restoration, image analysis and image synthesis (see Figure 2.1). These processes are explained in the following sections.

Image Acquisiti on (Camera)

Image Enhancement (Increasing Brightness & Contrast)

Image Restoration (Removing lens Distortion)

Image Analysis (Area of interest on image)

Image Synthesis (Shape reconstruction)

Figure 2.1 : Block diagram showing the stages in Digital Image Processing

6

2.1.1 Image Acquisition
A scene captured by some imaging device that converts the received light to an electrical signal. This electrical signal cannot directly be handled by digital circuits so the signals are converted to a discrete form by a digitizer. The resulting image can then directly be used in digital image processing applications. The digitizer performs sampling and quantization tasks. The values of continuous signals are sampled at specific locations in the image. After sampling the signals real values are discretized into digital numbers in the quantization process which gives a digital image (Baxes, 1994). The two major steps involved in image acquisition are choosing a camera and choosing the acquisition mode. The choice of choosing a camera may be difficult since there are so many different options. The two main types of camera are analog and digital camera. Analog cameras are cameras that generate a video signal in analog format. Digital cameras have several advantages over analog cameras. By digitizing at the camera level rather than at the image acquisition device, the signalto-noise ratio is typically higher, resulting in better accuracy. Digital cameras uses charge coupled device (CCD) for acquiring images. A Charge coupled device (CCD) is an array of interconnected photosensitive elements that stores and transfers its charge to a shift register, which converts the spatial array of charges in the CCD imager into a time-varying video signal. Timing information for the vertical and horizontal positions of the CCD and the sensor value combine to form the video signal. The various acquisition modes available are grab mode, snap mode, ring mode and sequence mode. The processing and display can then take place after the acquisition is complete.

2.1.2 Image Enhancement
The process of improving the quality of a digitally stored image by manipulating the image is image enhancement. To make an image lighter or darker, or to increase or decrease contrast, or reduce its noise contents, are examples of image enhancement. Image enhancement techniques are often used to improve an

7 image for a subsequent image analysis operation. Image enhancement techniques may be grouped in to subjective enhancement or objective enhancement (Baxes, 1994). Subjective enhancements are used to make an image visually appealing for subsequent processing. A sharpening operation may be applied to improve the appearance of the image. This may be repeated until the image yields the details necessary for the particular application. Objective image enhancement is the method to enhance the details of an image for particular area in digital image. These enhancements need not be visually appealing. Edge enhancement is one of the processes wherein only edge details are highlighted. In this research the edge enhancement is used for finding the coordinate centre of the scene image as explained in chapter 4.

2.1.3 Image Restoration
Image restoration removes or minimizes some known distortions in an image. Image restoration techniques may be used to restore images with problems like geometric distortion, improper focus and camera motion (Gonzalez and woods, 1993). Sometimes the distortion induced into an image may not be known. In these cases it may be possible to estimate how the image was distorted and recreate a close approximation of the original. Photometric restoration corrects an image for poor intensity responses. Similarly geometric distortions can be caused by the imaging system and also by the camera lens. One such distortion by camera lens is pincushion or barrel distortion. This distortion makes the image appear bloated in the centre. A method to restore this kind of image is discussed in section 2.7. Images with misfocus appear fuzzy, with a lack of detail. By making assumptions about the blurring function that caused the distortion, it is possible to remove the distortion by applying inverse filtering process.

2.1.4 Image Analysis
Image analysis combines techniques that compute statistics and measurements based on the grey-level intensities of the image pixels. The aim of image analysis is to understand an image by separating the elements of interest, generally objects in an

8 image. Their quantification includes such things as measure of size, range and description of outlines. Image analysis processes can subdivided into three steps. The first is image preprocessing, in which useless and distracting information from the image is removed. Care must be taken that this preprocess operation must not degrade the image in ways that interferes with the overall image analysis. The second stage is initial object discrimination, where objects with like characteristics are grouped separately. Each process in this stage works to isolate objects in an image, either by highlighting similar objects with a common brightness or by their edges. The final stage is object boundary cleanup, where object boundaries are reduced to single pixel widths. Image analysis algorithms for monochrome images are generally based on one of two basic properties of grey level values; namely discontinuity and similarity. The first approach is to partition an image based on abrupt changes in grey level. The areas of interest in this approach are detection of lines and edges in an image. The second approach is based on thresholding and region splitting or merging (Gonzalez and Woods, 1993). Image analysis also yields various image statistics. One important statistic is brightness hologram, which is the distribution of grey level in an image. This distribution is displayed in a graphical form. The histogram describes the overall or regional contrast attributes of an image, and can therefore be used to determine contrast enhancement parameters. Other image statistics such as the frequency content can also be useful information in carrying out the subsequent processes. In this research extracting the range information, avoiding the areas of less intensity near edges where the error is high are carried out through this image analysis process.

2.1.5 Image Synthesis
Image synthesis is the process where images are created from other image or non-image data. There are two primary forms of image synthesis operations (Baxes, 1994). The first is the reconstruction of any image using multiple projection images. The second form is visualization, where images are created for presentation purposes that may or may not be based on physical objects.

9 Images of three-dimensional objects can be created from multiple two dimensional images of the same scene. As long as the geometrical relationship between the images is controlled, the depth dimension can be recreated. This type of operation is used to synthesize images showing attributes that are not readily apparent in the original individual images. In particular three dimensional attributes such as depth features can be made more visible. In this thesis range images of the scene are captured at different views and are merged together by applying rotation and translation techniques to the data, to give complete three dimensional data. Using certain software this three dimensional data can be reconstructed into a complete model.

2.2 Image data handling
Image data handling has two major processes; image display and image storage. Image display is the process to viewing digital image on any display monitor, for this the digital image has to be converted back to an analogue video signal. The image display process is the reverse of the image digitization process. Image display is simple in comparison with the image digitization because of the necessary timing signals are already available from the digitization process. Image storage is the other critical process during image data handling. In some applications the digital image may be processed in real time (Baxes, 1994). In those cases the image data can flow directly from the digitizer, through the digital image processor and back out to the display function. But most of the applications require storing the image. It is because of reasons like processing is too complex to be carried out in real time or the processing operation requires access to all pixels in the image frame before processing can begin. There are two kinds of storage; the working image storage and permanent image storage. Working image storage is the process wherein images were stored temporarily. These are commonly referred to as image store. These have memory devices which can store the digitized video data produced by the image digitizer. The image data also leaves the image store at the same rate as they were stored to feed the image display circuitry. In permanent image storage, the processed digital image may be required for further processing or need to

10 be transferred to another digital image processing system so they were stored on permanent digital storage medium. It is necessary to store the digital image in a standardized format before archiving.

2.3 Range imaging
Range images are a special class of digital image. Range images have pixel values that represent the distance from the point of observation to the point on the object. Range images contain information that can be interpreted to give the three dimensional layout of the part of the object visible to the range camera. Range images are also referred to as depth images, depth maps, surface profiles and 2.5D images. In older range image cameras, the pixels had a depth of 8 bits. This was found to be so coarse that quantisation errors had a significant effect on the processing. More recent camera designs have given data with depths of 10, 12 or 16 bits per pixel (Russ, 2002). There are two main methods of collecting range images that are known as time of flight and structured lighting. The time of flight method uses a technique similar to radar. A pulse of laser light is directed to a point on the object. The time taken for the pulse to travel outwards to the object plus that for the reflected pulse to return to the camera is measured. The distance that the pulse travels can then be calculated. Range images can be represented in two basic forms. One is a list of 3D coordinates in a given reference frame (cloud of points), for which no specific order is required. The other is a matrix of depth values of points along the directions of the x, y image axes, which makes spatial organisation explicit. Range images are acquired with range sensors. Range sensors can be usefully characterised as active and passive range sensors. Active range sensors project energy (e.g. light) on 