Department of Computer Science

Hamilton, New Zealand

Automating iterative tasks with programming by demonstration

Gordon W. Paynter

This thesis is submitted in partial fulfilment of the requirements for the degree of Doctor of Philosophy at The University of Waikato.

February 26, 2000 Â© 2000 Gordon W. Paynter

Abstract

iii

Abstract

Programming by demonstration is an end-user programming technique that allows people to create programs by showing the computer examples of what they want to do. Users do not need specialised programming skills. Instead, they instruct the computer by demonstrating examples, much as they might show another person how to do the task. Programming by demonstration empowers users to create programs that perform tedious and time-consuming computer chores. However, it is not in widespread use, and is instead confined to research applications that end users never see. This makes it difficult to evaluate programming by demonstration tools and techniques. This thesis claims that domain-independent programming by demonstration can be made available in existing applications and used to automate iterative tasks by end users. It is supported by Familiar, a domain-independent, AppleScript-based programming-by-demonstration tool embodying standard machine learning algorithms. Familiar is designed for end users, so works in the existing applications that they regularly use. The assertion that programming by demonstration can be made available in existing applications is validated by identifying the relevant platform requirements and a range of platforms that meet them. A detailed scrutiny of AppleScript highlights problems with the architecture and with many implementations, and yields a set of guidelines for designing applications that support programming-by-demonstration systems and other agents. An evaluation shows that end users are capable of using programming by demonstration to automate iterative tasks. However, the subjects tended to prefer other tools, choosing Familiar only when the alternatives were unsuitable or unavailable. Familiar's inferencing is evaluated on an extensive set of examples, highlighting the tasks it can perform and the functionality it requires.

Acknowledgments

v

Acknowledgments

I sometimes wonder what I would be doing now if Ian Witten had not talked me into writing a thesis. (I imagine I would have cashed in my stock options and retired by now.) I also wonder what might have become of me had I attempted a thesis under a supervisor less generous with his time and experience than Ian, without whose encouragement and example I surely would never have finished. Thanks Ian. My two other supervisors, Geoff Holmes and Mark Apperley, have been very supportive, and patient. I'd like to thank them, and those others who have read this thesis and offered advice: Sally Jo Cunningham, Steve Jones, Simon Christiansen, Stuart Yeates, and Grant Floyd; and also my mother (who made one helpful suggestion before falling asleep at Section 1.3). I'm grateful to the many people who have commented on, and contributed to, my research over the last four years, including Allen Cypher, Eibe Frank, Craig Nevill-Manning, Carl Gutwin, Henry Lieberman, the staff of my department, and numerous others. I have borrowed code and ideas from any number of people, including Tom Bonura, Roman Zeiliger, Marty Geier (and his colleagues at Mitre), and the Weka programmers. I am sure I have omitted numerous others; for this I apologise. I would surely not have lasted four years without the diversions afforded me by my flatmates: Simon, Sarah, Dale, Teri, Donna, Cat, Nick, Jan, Elliot, and others too numerous to mention. Our department is blessed with a fantastic soccer team, comprising many of our students and staff. Thanks Alvin (who started it all), Annette, Aziz, Carl, Chithiran, Conrad, David B., David M., Dong, Eibe, Geoff, Jamie, Len, Max, Kim, Richard, Rob, Steve, Stuart, Talia, Tony, Yong... Finally, I'd like to thank my parents, my brother Craig, my sister Sharon, and the rest of my family, particularly the Gordons, for their support and encouragement. I dedicate this thesis, and the years of work it represents, to my uncle Doug Gordon, without whose support it could never have been attempted, let alone finished.

Table of contents

vii

Table of contents

Abstract Acknowledgements List of Figures List of Tables Chapter 1
1.1 1.2

iii v xv xix 1

Introduction

Programming by demonstration (PBD) ...............................................1 Thesis statement.......................................................................................3 1.2.1 1.2.2 1.2.3 Automating iterative tasks ...................................................4 End users and applications ..................................................5 Domain independence..........................................................5 Image transfer and conversion ............................................7 Averaging column data ........................................................8 Indexing document files .......................................................9

1.3

Examples of iterative tasks.....................................................................6 1.3.1 1.3.2 1.3.3

1.4

Thesis structure......................................................................................10

viii

Table of contents

Chapter 2
2.1 2.2 2.3 2.4

Background

13

Definitions and usage........................................................................... 14 Automating iteration ............................................................................ 21 PBD and iteration problems ................................................................ 24 PBD and repetition problems.............................................................. 27 2.4.1 2.4.2 2.4.3 Macro recorders .................................................................. 28 Extending macro recorders ............................................... 28 Editable histories................................................................. 30

2.5 2.6

Other demonstrational interfaces ....................................................... 31 Discussion and criticism of PBD......................................................... 34 2.6.1 2.6.2 2.6.3 Problems with agents ......................................................... 34 Problems with PBD............................................................. 35 Challenges for PBD research ............................................. 37

2.7

Summary ................................................................................................ 38

Chapter 3
3.1

Design considerations
3.1.1 3.1.2 3.1.3

39

Existing PBD designs............................................................................ 40 Feature-oriented designs ................................................... 41 User-oriented designs......................................................... 41 Successful end-user programming................................... 43 End user motivations.......................................................... 44 End user abilities................................................................. 45 End user attitudes ............................................................... 45 Design guidelines................................................................ 47 Arranging files..................................................................... 52 Arranging files when Familiar makes an error .............. 54 Sorting files .......................................................................... 56 Converting images.............................................................. 59 Converting images with the evaluation interface .......... 61

3.2

PBD for the end user............................................................................. 44 3.2.1 3.2.2 3.2.3 3.2.4

3.3

The Familiar user interface .................................................................. 51 3.3.1 3.3.2 3.3.3 3.3.4 3.3.5

3.4 3.5

Familiar's limitations............................................................................ 63 Summary ................................................................................................ 64

Table of contents

ix

Chapter 4
4.1 4.2

The Familiar architecture

65

System overview....................................................................................67 The event recorder.................................................................................70 4.2.1 4.2.2 4.2.3 AppleScript...........................................................................70 Application knowledge ......................................................71 Background knowledge......................................................76

Application independence ..............................................................77 4.3 Sequence recognition ............................................................................78 4.3.1 4.3.2 4.4 4.4.1 4.4.2 4.4.3 4.5 Detecting patterns................................................................78 Sequence recognition schemes ..........................................80 Evaluating competing predictions ....................................82 Explanations .........................................................................82 Pattern analysis schemes ....................................................84

Pattern analysis ......................................................................................81

Summary.................................................................................................89

Chapter 5
5.1

Machine Learning
5.1.1 5.1.2 5.1.3 5.1.4

91

Training and using classifiers ..............................................................92 The classification problem..................................................93 Trees and rules .....................................................................93 Training, testing, and prediction.......................................95 Machine learning algorithms.............................................95 Sequence recognition ..........................................................97 Pattern analysis ....................................................................99 Evaluation...........................................................................102 Discussion...........................................................................106 Finding attributes in instance information ....................107 Learning conditional rules with 1R ................................109 Permutation tests ...............................................................111 Discussion...........................................................................113

5.2

Guiding prediction ................................................................................96 5.2.1 5.2.2 5.2.3 5.2.4

5.3

Learning conditional rules .................................................................107 5.3.1 5.3.2 5.3.3 5.3.4

5.4

Summary...............................................................................................114

x

Table of contents

Chapter 6
6.1 6.2

Platform requirements

115

Requirements....................................................................................... 116 Command architectures..................................................................... 118 6.2.1 6.2.2 6.2.3 6.2.4 6.2.5 Low-level events ............................................................... 119 High-level events .............................................................. 120 Mid-level events................................................................ 121 Event hierarchies............................................................... 125 Alternatives to command architectures......................... 126 Application knowledge.................................................... 126 Sources of class information............................................ 127 Sources of instance information...................................... 129 User and task knowledge ................................................ 131 Background knowledge ................................................... 132 Start recording, stop recording ....................................... 132 Feedback language ........................................................... 133 Low-level and artificial syntax........................................ 133 English-like text and natural language syntax ............. 134 Graphically representing objects .................................... 134 Graphically representing commands............................. 135 Anticipation and highlighting......................................... 136 Guide objects and ghost objects...................................... 136 Forms and dialog boxes ................................................... 137

6.3

Detailed inferencing requirements................................................... 126 6.3.1 6.3.2 6.3.3 6.3.4 6.3.5

6.4

Detailed user interface requirements ............................................... 132 6.4.1 6.4.2 6.4.3 6.4.4 6.4.5 6.4.6 6.4.7 6.4.8 6.4.9

6.4.10 Buttons, menus, and speech ............................................ 137 6.5 Case study: AppleScript..................................................................... 138 6.5.1 6.5.2 6.5.3 6.5.4 6.5.5 6.5.6 6.6 Why AppleScript is used in Familiar............................. 139 Problems with high-level event architectures .............. 140 Problems with the AppleScript language ..................... 142 Problems with AppleScript implementations .............. 145 Other issues........................................................................ 148 Guidelines for PBD-aware scriptable application........ 150

Summary .............................................................................................. 153

Table of contents

xi

Chapter 7
7.1

Evaluation
7.1.1 7.1.2 7.1.3 7.1.4 7.1.5 7.1.6

155

User evaluation ....................................................................................157 Procedure............................................................................157 Observations.......................................................................162 Results .................................................................................163 Testing the end user's ability to use PBD.......................164 Testing the end user's tool preference............................164 User Feedback ....................................................................166 Procedure............................................................................169 Results .................................................................................172 Shortcomings of application programs ..........................174 Shortcomings of Familiar .................................................177

7.2

Task evaluation ....................................................................................169 7.2.1 7.2.2 7.2.3 7.2.4

Chapter 8
8.1 8.2

Conclusions

187

Overview...............................................................................................187 Summary of contributions..................................................................189 8.2.1 8.2.2 8.2.3 Making PBD available in existing applications.............189 End users can automate iteration with PBD..................190 Familiar ...............................................................................191

8.3 8.4

Claims revisited ...................................................................................192 Future work..........................................................................................193 8.4.1 8.4.2 8.4.3 8.4.4 Augmenting Familiar........................................................193 Machine learning ...............................................................196 User studies ........................................................................197 Improved computer architectures...................................197

References

199

xii

Table of contents

Appendix A Iterative tasks
A.1 A.2 A.3 A.4 A.5 A.6 A.7 A.8 A.9

209

Averaging column data...................................................................... 209 Calendar ............................................................................................... 210 Copying files........................................................................................ 210 Copying files to floppy....................................................................... 210 Copying mail headers ........................................................................ 211 Image conversion ................................................................................ 211 Indexing document files .................................................................... 212 Joining document sections................................................................. 213 Mail merge ........................................................................................... 214

A.10 Network diagram................................................................................ 214 A.11 Numbering table of contents............................................................. 215 A.12 Printing odd and even pages ............................................................ 216 A.13 Program editing .................................................................................. 217 A.14 Saving search results .......................................................................... 218 A.15 Sorting rectangles................................................................................ 219 A.16 Subtotal................................................................................................. 219 A.17 Truncate lines ...................................................................................... 220 A.18 Discarded tasks ................................................................................... 220 A.18.1 Fractal snowflake .............................................................. 221 A.18.2 Intelligent image filtering ................................................ 222 A.18.3 Manipulating checklists ................................................... 222

Appendix B Training tasks
B.1 B.2 B.3 B.4 B.5 B.6 B.7

225

Label by size......................................................................................... 225 Label by kind ....................................................................................... 225 Hendry & Green.................................................................................. 225 Hendry & Green extended ................................................................ 226 Resize tiles............................................................................................ 226 Move and rename files ....................................................................... 226 Position files......................................................................................... 226

Table of contents

xiii

Appendix C Attributes for PAS-ML Appendix D User evaluation instructions

227 231

D.1 The pre-experiment questionnaire .......................................................232 D.2 The Familiar tutorial...............................................................................233 D.3 Additional Familiar instructions ..........................................................239 D.4 Calendar task instructions.....................................................................241 D.5 Image task instructions ..........................................................................244 D.6 Post-experiment interview questions ..................................................246

Appendix E Generating AppleScript code
E.1 E.2 E.3

247

Generation algorithm..........................................................................248 A worked example ..............................................................................248 Discussion.............................................................................................250

List of figures

xv

List of figures

Figure 1.1 Figure 1.2 Figure 3.1 Figure 3.2 Figure 3.3 Figure 3.4 Figure 3.5 Figure 3.6 Figure 3.7 Figure 3.8 Figure 3.9

Performing Harvey's file transfer task (a) one file at a time, and (b) applying each action in turn to every file.................................................8 Sue's finished spreadsheet, showing sample data (columns A and B) and the required formulas in Microsoft Excel format (column C) .......9 The Familiar menu.....................................................................................52 The screen before the arranging files task..............................................53 Using Familiar to arrange files.................................................................54 The screen after two demonstrations of the arranging files task........55 Completing an iterative task with Familiar ...........................................56 Changing an incorrect cycle .....................................................................57 Examining and changing an incorrect prediction.................................58 Changing an incorrect cycle .....................................................................59 Changing an incorrect parameter............................................................60

Figure 3.10 Converting image files ..............................................................................61 Figure 3.11 Converting image files using the evaluation interface.........................62 Figure 4.1 Figure 4.2 Figure 4.3 Figure 4.4 Figure 4.5 The Familiar architecture..........................................................................68 AppleScript recorded in Microsoft Excel ...............................................71 Familiar's model of the Finder (version 8.1)..........................................73 The Familiar model of Microsoft Excel (version 5.0)............................74 Templates for AppleScript commands to (a) get properties, (c) evaluate a property, and (e) get containee objects, with (b,d,f) examples of each ........................................................................................75 Familiar's background knowledge..........................................................77 Rules for estimating the probability a pattern is correct......................79 Candidate patterns found by SRS-noisy in the event trace shown in Figure 3.10a,b..............................................................................................81 Rules for estimating the probability that a prediction is correct ........83

Figure 4.6 Figure 4.7 Figure 4.8 Figure 4.9

Figure 4.10 An event trace from Microsoft Excel.......................................................85 Figure 4.11 Excerpts from the event trace in Figure 3.8 and 3.9..............................88

xvi

List of figures

Figure 4.12 A rule for predicting the to parameter based on the file type of the selection ...................................................................................................... 90 Figure 5.1 Figure 5.2 Figure 5.3 Figure 5.4 Figure 5.5 Figure 5.6 Figure 5.7 Figure 5.8 Decision tree for the weather problem................................................... 94 Predicting the class of the weather problem with (a) a textual representation of a tree, and (b) an equivalent rule ............................. 95 Attributes of candidate patterns ............................................................. 98 Decision tree for predicting whether a pattern is correct.................... 99 Attributes used to build the model of prediction correctness .......... 101 Decision tree for classifying parameter predictions........................... 103 An event trace in the Finder .................................................................. 109 Rules for predicting the to parameter based on (a) the current selection, (b) the file type of the selection, and (c) the size of the selection .................................................................................................... 111 A lowÂ­level event trace ......................................................................... 120 A high-level AppleScript event trace recorded in Microsoft Excel . 121 A macro recorded in Microsoft Word in the Visual Basic language122 A mid-level event trace recorded by WOSIT (adapted from Geier, 1999, pp. 16Â­17) ....................................................................................... 123 A mid-level event trace recorded by Topaz (adapted from Myers, 1998, p. 537).............................................................................................. 124 A simple iterative task recorded in AppleScript ................................ 129 Representations of a file object and its size property following the style of Familiar, SmallStar, and Pursuit ............................................. 135 The Finder (version 8.1) dictionary entry for a disk object ................ 141 The Finder (version 8.1) dictionary entry for the set command ....... 142 A subject completing the image conversion task with Familiar.......... 159 The tools used by the nine users (AÂ­I) completing the seven steps (1Â­7) of variant 4 of the image conversion task...................................... 163 The copying files to floppy task ................................................................ 179 Event traces recorded while printing pages 1, 3, and 5 of a document in (a) Microsoft Excel, (b) the Scriptable Text Editor, and (c) Microsoft Word ....................................................................................... 180 Event trace for the copying mail headers task ........................................ 181 Automating the joining document sections task (a) with regular URLs in Netscape Navigator and (b) with a hypothetical web browser... 182 Completing the mail merge task with Familiar.................................. 183 Using Familiar to (a,b) successfully change macro names, and (c,d) unsuccessfully attempt to infer irregular spreadsheet regions ........ 184 Pseudocode for automating the subtotal task ...................................... 186

Figure 6.1 Figure 6.2 Figure 6.3 Figure 6.4 Figure 6.5 Figure 6.6 Figure 6.7 Figure 6.8 Figure 6.9 Figure 7.1 Figure 7.2 Figure 7.3 Figure 7.4

Figure 7.5 Figure 7.6 Figure 7.7 Figure 7.8 Figure 7.9

List of figures

xvii

Figure A.1 The calendar to be duplicated in the calendar task (reproduced in full in Appendix D.4) ...................................................211 Figure A.2 The copying mail headers task, showing (a) the data and (b) the finished list (adapted from Cypher, 1993b) .........................................212 Figure A.3 Data for the mail merge task, showing (a) part of the address list, and (b) the form letter .....................................................................................215 Figure A.4 Data for the network diagram task...........................................................216 Figure A.5 Part of the finished network diagram. .....................................................217 Figure A.6 One chapter from the numbering table of contents task, (a) before and (b) after completion .................................................................................218 Figure A.7 The Step2() macro for the program editing task ....................................219 Figure A.8 AltaVista search results for "link:ps.Z" in text mode used in the saving search results task ..........................................................................220 Figure A.9 The subtotals task, (a) before, and (b) after (with formulas displayed)221 Figure A.10 The transformation applied in the fractal snowflake task ....................222 Figure A.11 Part of the data for the manipulating checklists task .............................233 Figure E.1 The algorithm for generating AppleScript code from Familiar........248 Figure E.2 An AppleScript program that automates the arranging files task (Section 3.3.1)............................................................................................249

List of tables

xix

List of tables

Table 3.1 Table 4.1 Table 4.2 Table 4.3 Table 5.1 Table 5.2 Table 5.3 Table 5.4 Table 6.1 Table 7.1 Table 7.2 Table 7.3 Table 7.4 Table 7.5

Guidelines for designing end-user PBD.................................................48 Application size measured by quantity of high-level commands, classes (ignoring plurals), and enumerations........................................72 Summary of the pattern analysis schemes.............................................86 Contextual data for the to parameter in Figure 4.11 .............................89 A simple machine learning problem (adapted from Quinlan, 1986) .93 Evaluation dataset statistics ...................................................................104 Comparison of techniques for choosing the best prediction.............105 Contextual data for the to parameter in Figure 5.7 .............................110 Platform requirements of domain-independent PBD systems .........117 Experimental procedure for the user evaluation ................................158 The number of subjects with experience in each of the domains and applications in the user evaluation .......................................................160 Participation rates in the user evaluation.............................................161 Tool preference in variant 4 of the user evaluation ............................166 A summary of the tasks in Appendix A, showing the source, the domain, and a short description of each (tasks 18Â­20 are not used in the task evaluation) .................................................................................171 Source and domain of example tasks ...................................................172 Requirements for automating the example tasks with Familiar.......173 Applications used in the example tasks ...............................................174 Tasks that Familiar is able to learn, that the inferencing model is able to learn, and that are beyond the inferencing model .........................175

Table 7.6 Table 7.7 Table 7.8 Table 7.9

1 Introduction

Computers reputedly excel at repetitive tasks, yet users are frequently forced to perform the same actions over and over again. Repetition can be automated with computer programs, but this is no solution for the majority of users, who have neither the time nor the inclination to learn computer programming. They have little choice but to perform repetition by hand. Programming by demonstration is an end-user programming technique that lets users create programs by showing the computer examples of what they want to do. The computer observes the user's demonstration and attempts to learn a solution to their problem. When the task is mastered, the user can delegate its completion to the computer. An underlying assumption is that a user who knows enough to perform a particular task knows enough to teach the task to another person by showing them how it is performed. The goal of programming by demonstration is to learn from such a performance, just as another human would, so that the user can instruct the machine even if they do not have the requisite skills to write a computer program. The advantage of this approach is that the user assumes the role of a teacher, not a programmer, and relies on everyday teaching skills, not rarefied programming expertise. In an ideal world, non-programmers would use programming by demonstration to automate tasks they would otherwise perform by hand. Sadly, the real world is less accommodating, and programming by demonstration is unavailable to the majority of users. This thesis shows how it could flourish on every desktop.

1.1 Programming by demonstration (PBD)
The macro recorder is the simplest and most widely used form of PBD. A macro is a simple program. A macro recorder uses a "tape recorder" metaphor to create a program: the user instructs the system to start recording, performs the actions

2

Chapter 1: Introduction

they wish to record, and instructs the system to stop recording. The user is asked to assign their macro a name and method of invocation, typically a menu item or combination of keystrokes. Later, the user can invoke the macro, and the actions they "recorded" will be "played back" in the user interface. Consider, for example, a businessman who, at the end of every day, copies the file