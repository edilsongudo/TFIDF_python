Reinforcement Learning for Racecar Control
Ben Cleland

This thesis is submitted in partial fulfillment of the requirements for the Degree of Master of Science at the University of Waikato. March 2005 ­ March 2006 © Ben Cleland 2006

ii

"The only true wisdom is in knowing you know nothing." Socrates

Abstract
This thesis investigates the use of reinforcement learning to learn to drive a racecar in the simulated environment of the Robot Automobile Racing Simulator. Real-life race driving is known to be difficult for humans, and expert human drivers use complex sequences of actions. There are a large number of variables, some of which change stochastically and all of which may affect the outcome. This makes "driving" a promising domain for testing and developing Machine Learning techniques that have the potential to be robust enough to work in the real world. Therefore the principles of the algorithms from this work may be applicable to a range of problems. The investigation starts by finding a suitable data structure to represent the information learnt. This is tested using supervised learning. Reinforcement learning is added and roughly tuned, and the supervised learning is then removed. A simple tabular representation is found satisfactory, and this avoids difficulties with more complex methods and allows the investigation to concentrate on the essentials of learning. Various reward sources are tested and a combination of three are found to produce the best performance. Exploration of the problem space is investigated. Results show exploration is essential but controlling how much is done is also important. It turns out the learning episodes need to be very long and because of this the task needs to be treated as continuous by using discounting to limit the size of the variables stored. Eligibility traces are used with success to make the learning more efficient. The tabular representation is made more compact by hashing and more accurate by using smaller buckets. This slows the learning but produces better driving. The improvement given by a rough form of generalisation indicates the replacement of the tabular method by a function approximator is warranted. These results show reinforcement learning can work within the Robot Automobile Racing Simulator, and lay the foundations for building a more efficient and competitive agent.

iii

iv

Acknowledgements
To my main supervisor Dr Tony Smith, for the original idea of using a racecar simulator; for lots of lively dialogue and debate; for your enthusiasm; for believing in me even when I did not; for patience and understanding; and for sticking by me. During the dread write-up process you stated you were the devil's advocate, and now I know what you meant! To my other main supervisor Dr Bernhard Pfahringer for many long discussions and for looking after me when Tony was overseas, and also for providing financial support from your own research account. I discovered another of your talents when I lost against you in an arm wrestle! And to Dr Kurt Driessens, our visiting post-doc and local expert on reinforcement learning, for setting me straight on a number of important theoretical matters. I wish I had pestered you more while you were here, so you were fortunate. Your skill at drawing diagrams may be appalling but you can certainly think clearly! Special thanks to my long suffering Mum. An apology to Richard Sutton: The term "Q-value" 1 has been used frequently in this work! However, the more helpful terms "action value", "state-action value", "expected-sum-offuture-rewards" and even "sum of rewards historically-available/expected from a given state-action-pair" are also used from time to time throughout, as an equivalent.

1

Sutton's compelling disparagement of the term Q-value is at:

http://www.cs.ualberta.ca/~sutton/RL-

FAQ.html#Q-value

v

vi

Contents
Abstract.................................................................................................................................iii Acknowledgements................................................................................................................v List of Figures.......................................................................................................................xi List of Tables ......................................................................................................................xiv 1 Introduction....................................................................................................................1 1.1 1.2 1.3 1.4 2 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 3 3.1 3.1.1 3.1.2 3.2 3.3 3.4 3.5 3.6 3.7 3.7.1 3.8 3.8.1 3.8.2 Context...................................................................................................................1 The Problem Domain.............................................................................................3 The General Approach...........................................................................................6 Thesis Outline ........................................................................................................7 Reinforcement Learning Defined ..........................................................................9 Previous Work on Learning within RARS ..........................................................11 Remaining Questions in the Field of Reinforcement Learning ...........................16 Objective..............................................................................................................23 Crash Recovery....................................................................................................23 State Description Parameters ...............................................................................25 Measurement of Success......................................................................................27 Chapter Summary ................................................................................................29 Tabular Method....................................................................................................31 Reasons for using a Tabular Method ........................................................... 31 Reasons for using Initial Supervised Learning (with the tabular method)... 32 Preliminary setup .................................................................................................33 Rewards ...............................................................................................................36 Temporal Difference Backups (1 Step) ...............................................................42 Proof of Concept, using damage reward only......................................................44 Lap Time Reward ................................................................................................46 Combined Lap Time and Damage Reward..........................................................47 Medium Term Trend .................................................................................... 48 Exploration...........................................................................................................50 Exploration, when both Lap time and Damage Rewards are used .............. 52 Random Proportional Selection ................................................................... 56 vii

Background....................................................................................................................9

Primed Learning ..........................................................................................................31

3.9 3.9.1 3.9.2 3.9.3 3.9.4 3.9.5 3.9.6 3.10 4 4.1 4.2 4.3 4.3.1 4.3.2 4.3.3 4.3.4 4.3.5 4.4 5 5.1 5.1.1 5.1.2 5.1.3 5.2 5.2.1 5.2.2 5.2.3 5.2.4 5.3 5.3.1 5.3.2 5.3.3 5.3.4 5.4 5.4.1

Non-scheduled Exploration..................................................................................58 Medium Term Effect of No Exploration, Learning Rate 0.01 (Primed learning) ....59 Reducing Inherent Exploration ....................................................................60 Shaping? .......................................................................................................66 The Wider Picture ........................................................................................66 Backups when Exploring..............................................................................67 Exploration needs Further Reduction in the Long-term...............................69 Chapter Summary ................................................................................................71 Discontinuing Initial Supervised Learning ..........................................................73 Effects of Initial/Default Q-value ........................................................................76 Discounting ..........................................................................................................80 Motivation for Using Discounting ...............................................................80 Discount of 0.99 ...........................................................................................85 Discount of 0.9 .............................................................................................88 Discount of 0.8 .............................................................................................91 Discount of 0.995 .........................................................................................94 Chapter Summary ................................................................................................95 Altering The Damage Rewards............................................................................97 Increasing the Damage Reward....................................................................97 Decreasing the Damage Reward ..................................................................98 Distribution of Damage Rewards in the Final Scheme Used.....................100 Speed Rewards...................................................................................................101 Speed Reward Schemes..............................................................................102 The Lap-time / Damage Balance is Altered ...............................................104 The Long-term Effect.................................................................................106 Speed Rewards may be Ambiguous...........................................................108 Eligibility Traces................................................................................................110 Method Used ..............................................................................................111 A Peculiarity of RARS ...............................................................................112 Medium-term Effects .................................................................................113 Lengths of Eligibility Traces......................................................................114 Discretisation .....................................................................................................115 Hashing.......................................................................................................115 viii

Unprimed Learning and Continuous Learning ............................................................73

Rewards and Efficiency ...............................................................................................97

5.4.2 5.5 5.5.1 5.5.2 5.5.3 5.6 6 6.1 6.1.1 6.1.2 6.1.3 6.1.4 6.1.5 6.2 6.2.1 6.2.2 6.2.3 6.2.4 6.2.5 6.3 6.4 7 7.1 7.2

Discretisation Schemes .............................................................................. 117 Generalisation (Nearest Neighbour) ..................................................................120 Generalisation, Long-term Effect (8,000,000 laps) ................................... 123 Generalisation, Early Learning Effects (100,000 laps).............................. 125 Generalisation to Other Tracks .................................................................. 126 Chapter Summary ..............................................................................................127 Domain (RARS) Effects ....................................................................................129 Effects of back-up routines that occur after crashes .................................. 132 Graphing Total-rewards ............................................................................. 135 Damage Rewards given During Pit Stops .................................................. 136 Back-ups on Pit Entry and Exit.................................................................. 137 Eligibility Traces must Not Bridge the Pits ............................................... 139 Analysis of Model, Learning and Performance .................................................140 Q-value Ranges .......................................................................................... 141 Final Q-value Distributions........................................................................ 142 Statistical Presentation: Sampling versus Averaging Graphs .................... 146 Number of Visits to State-action-pairs....................................................... 148 Proportion of State-action-pairs Visited, with less than five previous visits .. 155 Screen Shots.......................................................................................................157 Chapter Summary ..............................................................................................159 Summary............................................................................................................161 Future Work.......................................................................................................164

Domain Effects; Model Analysis; Screen-shots ........................................................129

Summary and Future Work........................................................................................161

References..........................................................................................................................167 Appendices.........................................................................................................................175 A. Comparison of Q-learning and Sarsa.......................................................................175 B. Inputs and Outputs of a RARS Robot Driver...........................................................176

ix

x

List of Figures
Figure 2-1 The Agent-environment Interaction (Reproduced from [Sutton and Barto, 1998])...9 Figure 2-2 The Crash Recovery Set-up ............................................................................. 24 Figure 2-3 A Robot Takes a Situation Input and Produces an Action Output................... 26 Figure 2-4 A Superb Learning Curve when the goal is Minimisation............................... 28 Figure 3-1 The Derivation of Tangential Velocity ............................................................ 40 Figure 3-2 Lap Time, 3,000 laps, Reward on Crash only.................................................. 45 Figure 3-3 Lap Time, 3,000 laps, Reward on Lap Time Only .......................................... 46 Figure 3-4 Lap Time, 3,000 laps, Reward on Lap Time and Crash .................................. 47 Figure 3-5 Lap Time, 100,000 laps, Reward on Lap Time and Crash .............................. 49 Figure 3-6 Lap Time, 3,000 laps, Lap Time Rewards, 0.1% Random Actions................. 51 Figure 3-7 Lap Time, 100,000 laps, Lap Time and Damage Rewards, 0.1% Exploration........... 53 Figure 3-8 Lap Time, 1,000,000 laps, Lap Time and Damage Rewards, 0.1% Exploration...... 54 Figure 3-9 Lap Damage, 1,000,000 laps, Lap Time and Damage Rewards, 0.1% Exploration . 55 Figure 3-10 Lap Time, 100,000 laps, Lap Time and Damage Rewards, 0.1% Random Proportional Selection .................................................................................... 58 Figure 3-11 Lap Time, 100,000 laps, No Exploration, Learning Rate 0.01 ...................... 59 Figure 3-12 Tie-breaking, by taking the Average of Equally Good Actions .................... 65 Figure 3-13 Exploration needs Further Reduction in the Long-term ................................ 70 Figure 3-14 Total Rewards per Lap, Exploration Decreasing to 0.003% at Lap 8,000,000 .. 71 Figure 4-1 Without Initial Supervised Learning................................................................ 74 Figure 4-2 Without Initial Supervised Learning................................................................ 75 Figure 4-3 Using an Initial Q-value of 400........................................................................ 77 Figure 4-4 Using an Initial Q-value of 100........................................................................ 77 Figure 4-5 Using an Initial Q-value of 30.......................................................................... 78 Figure 4-6 Divergence when: No Exploration; Learning Rate 0.01; Initial Q-value 400; =1.0 ... 81 Figure 4-7 No Divergence when: No Exploration; Learning Rate 0.1; Initial Q-value 400; =1.0 .. 82 Figure 4-8 "Inherent Exploration" Dissipates ................................................................... 84 Figure 4-9 No Divergence when: No Exploration; Learning Rate 0.01; Discounting of 0.99......... 85 Figure 4-10 Discount of 0.99: Lap Time Effect ................................................................ 86 Figure 4-11 Discount of 0.99: Damage Effect................................................................... 87 Figure 4-12 Discount of 0.9: Lap Time Effect .................................................................. 88 Figure 4-13 Discount of 0.9: Damage Effect..................................................................... 89 xi

Figure 4-14 Discount of 0.9: Lap Time Effect across 40,000,000 Laps............................90 Figure 4-15 Discount of 0.9: Damage Effect across 40,000,000 Laps ..............................90 Figure 4-16 Discount of 0.99: Total-rewards-per-lap Effect (Baseline for Figure 4-17) ..91 Figure 4-17 Discount of 0.8: Total-rewards-per-lap Effect ...............................................92 Figure 4-18 Discount of 0.8: Total-rewards-per-lap Effect, 20,000,000 Laps ..................93 Figure 4-19 Discount of 0.995: Total-rewards-per-lap Effect ...........................................94 Figure 5-1 Lap Time (Left), Lap Damage (Right), With Damage Reward of 44 to 237...99 Figure 5-2 Damage Rewards, Ordered by Size................................................................100 Figure 5-3 Lap times, from the Experiment of Figure 5-12 ............................................104 Figure 5-4 Lap times, with Speed Rewards used, 2,000,000 Laps ..................................105 Figure 5-5 Total Rewards, with Speed Rewards used, 2,000,000 Laps ..........................105 Figure 5-6 Lap times, with Speed Rewards used, 8,000,000 Laps ..................................107 Figure 5-7 Total Rewards, with Speed Rewards used, 8,000,000 Laps ..........................108 Figure 5-8 The First Use of an Eligibility Trace..............................................................112 Figure 5-9 Using an Eligibility Trace, with the Pit stop Damage Reward Ignored.........113 Figure 5-10 Performance when Hashing is used .............................................................116 Figure 5-11 Total Rewards, Before Increasing Discretisation Resolution ......................117 Figure 5-12 Total Rewards, After Increasing Discretisation Resolution.........................118 Figure 5-13 Lap Times when using Nearest Neighbour Generalisation..........................124 Figure 5-14 Total Rewards per Lap when using Nearest Neighbour Generalisation ......124 Figure 5-15 Total Rewards per Lap, Nearest Neighbour Generalisation, First 100,000 Laps ..125 Figure 5-16 Tracks v01.trk (left) and v03.trk (right) .......................................................126 Figure 6-1 Backups Across a Crash.................................................................................133 Figure 6-2 Performance With the Crash-backup Coding Oversight................................134 Figure 6-3 Performance With Backups-across-crashes executed correctly.....................134 Figure 6-4 Graphing Total-rewards-per-lap Gives the True Measurement of Learning Progress..136 Figure 6-5 Total Rewards per Lap Once the Pit Entry "Crash" is Ignored .....................137 Figure 6-6 Total Rewards per Lap when Learning Is Performed During Pit Entry and Exit.......138 Figure 6-7 Total Rewards per Lap when Learning Is Prevented During Pit Entry and Exit ..139 Figure 6-8 The Minimum and Maximum Q-value, Measured Each Lap, for 1,100 Laps .141 Figure 6-9 The Min. and Max. Q-value, Measured Each 1,000 Laps, for 2,000,000 Laps..142 Figure 6-10 The Distribution of Q-values, After 2,000 Laps, Discount 0.9....................143 Figure 6-11 The Distribution of Q-values, After 2,000,000 Laps, Discount 0.9.............144 Figure 6-12 The Distribution of Q-values, After 2,000,000 Laps, Discount 0.99...........145 Figure 6-13 Total Rewards per Lap, Using Sampling Every 100th Lap to Compress the Data ...147 xii

Figure 6-14 Number of Counts Versus Number of Previous Visits to State-actionpair, across 8M Laps............................................................................. 149 Figure 6-15 Number of Counts Versus Number of Previous Visits to State-actionpair, across First 400,000 Laps............................................................. 150 Figure 6-16 Number of Counts Versus Number of Previous Visits to State-actionpair, across Laps 1.6M to 2M............................................................... 151 Figure 6-17 Number of Counts Versus Number of Previous Visits to State-actionpair, across Laps 7.2M to 8M............................................................... 153 Figure 6-18 Proportion of State-action-pairs Visited That Have Less Than Five Previous Visits, across 320,000 Laps ..................................................... 155 Figure 6-19 Proportion of State-action-pairs Visited That Have Less Than Five Previous Visits, across 3.2M Laps.......................................................... 156 Figure 6-20 The First Corner of Track v01.trk................................................................ 157 Figure 6-21 The Second Corner of Track v01.trk ........................................................... 158 Figure 6-22 The Third Corner of Track v01.trk .............................................................. 158

xiii

List of Tables
Table 2-1 Explanation of the Environmental Parameters Selected as Inputs ....................27 Table 5-1 Effect of Increasing the Damage Reward..........................................................97 Table 5-2 Effect of Decreasing the Damage Reward ........................................................98 Table 5-3 Performance on The First 100 Laps of Track vo3.trk, Using The Model Learnt on Track v01.trk ..............................................................................127

xiv

1 Introduction
This thesis investigates the use of reinforcement learning to learn to drive a racecar in a simulated environment. The aim is to use Q-learning to learn to drive a robot racecar at a competitive pace in the simulated environment provided by the Robot Automobile Racing Simulator (RARS) [Timin, 1995]. Driving a car involves taking a set of sensory inputs and producing some control outputs. Driving at minimum lap time involves an optimisation of this process; and reinforcement learning provides a mechanism to achieve this automatically.

1.1 Context
The ultimate aim of artificial intelligence is to match or exceed the intelligence of humans. Whether or not this is even theoretically possible is a matter of long debate. A widely agreed definition of the term "intelligence" is also elusive. For a discussion of this issue see the introduction and conclusion of any artificial intelligence text (e.g. [Russell and Norvig, 2003]). However, some aspects of human intellect are easier to define, and would be clearly useful to imitate. One of these is the act of learning. It would be very useful for a machine to be able to learn in a manner similar to human learning. Such a machine could be placed in environments too dangerous for humans; or too difficult for humans; or set on tasks that no human has yet mastered. Various types of learning have been used during the history of artificial intelligence. One method by which a machine can learn is by interacting with its environment, observing the effect of its actions and discovering how to alter its actions to achieve some desired outcome. "Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence." [Sutton and Barto, 1998]. This is the idea behind Reinforcement Learning. A rough historical context of reinforcement learning follows. During the 1970s and 1980s "expert systems" were popular and are still used today. Expert systems encode the detailed knowledge of a human expert in some narrow domain. They typically include the "fuzzy" or "intuitive" knowledge gained by experts after long 1

experience. These systems are rule-based, painstaking to construct, require a large amount of the expert's time during construction and need updating as the (human) expert's knowledge grows. These systems make the expert's skills much more widely available. But they can not discover new knowledge; although there may be some "clean-up" effect through generalisation (similar to interpolation and extrapolation in statistics). The bottleneck in developing an expert system has proved to be the knowledge acquisition phase. Machine learning techniques relieve this bottleneck to a large extent. In general, machine learning requires a supply of correctly classified examples--for example, a situation along with the correct action to take, as judged by an expert. During the learning phase the machine learning algorithm automatically finds patterns in the input/output relationship and from this it (usually) forms a model. The model can then be applied to novel inputs (i.e. unclassified examples) to yield the correct outputs. Machine learning systems are much easier to build than are expert systems, in that: the expert needs only to supply correctly classified positive and negative examples; the expert's intuition is implicitly encoded; and the expert does not need to supply the reasoning behind the classifications (as is usually needed for setting up an expert system). This particular form of machine learning is known as "supervised learning". In some problem domains there is no expert; or the best "expert" is not very accomplished; or it is not certain that the best solution known is the actual optimum. In these sorts of situations a learning mechanism is needed that is able to discover new facts. One such method is Reinforcement Learning, a sub-branch of machine learning. Reinforcement Learning combines ideas from the much earlier work of Dynamic Programming (which was developed under the study of Control Theory) and Monte Carlo methods (from Statistics) [Sutton and Barto, 1998]. Evolutionary Methods (e.g. Genetic Algorithms) can also be viewed as a type of reinforcement learning [Thrun and Littman, 2000]. Reinforcement learning, within machine learning, is related to the concept of reinforcement learning in psychology. Reinforcement learning involves learning by trial and error by interacting with the environment in the domain of interest. The environment must provide feedback (called a "reinforcement" or "reward") and this may be delayed from the actions responsible (e.g. a simple "win" or "lose" at the end of a game). Reinforcement learning is goal-directed, 2

where the goal is to maximise the sum (or future-discounted sum) of numerical "rewards" that are received during an episode. This is done by learning how to map situations to the most suitable actions. With reinforcement learning no expert is needed; the method can continuously adapt if the task requirements vary (sometimes called "concept drift"); and new knowledge can be discovered. Prior domain knowledge, as is used during supervised learning or in setting up an expert system, can be incorporated in order to give the learning a head-start or to direct the learning. The most impressive example of reinforcement learning is possibly Gerry Tesauro's TDGammon. This plays backgammon at the level of the best human players in the world, and demonstrated some superior tactics that have now been adopted by the best human players. [Tesauro, 1994, 1995]. Success in other reinforcement learning endeavours has been more modest, however it is still early days in the history of reinforcement learning.

1.2 The Problem Domain
The problem domain addressed by this thesis is that of learning to drive a race car around a circuit in the minimum lap time. In real-life, this is known to be difficult for humans. The optimal actions are not exactly known because there are a very large number of variables, many of which often change (sometimes stochastically) and all of which may affect the outcome--for example: suspension geometries; road surface; temperature; air pressure; and tyre wear. Some of the best solutions found by expert human drivers use sequences of actions that may appear obscure. For example, these sometimes rely on side effects that only occur near the limit of the car's adhesion. These side effects may usually be a disadvantage but when used at the correct moment can counteract some other difficulty-- for example: using the instability, that occurs at the point of maximum braking, to turn the car into a corner; or using the rear drift imposed by the sweeping action of rally tyres under power on a loose surface to steer the car, when the (useful) extra traction gained by the sweeping action has actually reduced the amount of steering effect given by the front tyres. This shows that the optimal solution (at least, that known to human drivers) is sometimes in a remote corner of the search space. This makes it very difficult to learn excellent race driving. However, near-optimal solutions are well known, due to vast human experience in 3

the real world. Nevertheless, humans require procedural learning--that is, learning-bydoing--rather than factual learning, to be able to drive a car, ride a bicycle, touch type, and so forth. That is, being told how to ride a bicycle is not enough, it also requires physical experience, and therefore every human must learn from scratch. The difficulty of "driving" makes it a promising domain for testing and developing Machine Learning techniques that have the potential to be robust enough to work in the real world. This assumes a realistic simulator is available. Some early work in artificial intelligence that was successful in simple microworlds (sometimes called "toy" worlds) proved unscalable to more complex worlds, let alone the real world which is usually highly dimensional and often has all manner of confounding influences at work. For example, Terry Winograd's natural language understanding program works successfully in the "blocks" world, but due to its lack of general knowledge does not scale to the real world [Russell and Norvig, 2003]. The work presented in this thesis uses a simulator and therefore an artificial world; however, it is a reasonably complex world and is nearer to real life than some classic reinforcement learning domains such as the inverted pendulum and mountain car [Sutton and Barto, 1998; etc]. The simulator used in this work has 45 state parameters that describe the immediate environment, plus an additional 20 or more concerning nearby cars, many of which are continuous values (e.g. the speed of the car, the distance from the side of the track, etc). These can be used as inputs to agents within the world, and are all fully described in the Appendix. The simulator requires two simultaneous continuous actions (outputs) from each agent: steering-angle and desired-velocity. Furthermore, a proportion of the statechanges are executed randomly. The simulator used in this work is the Robot Automobile Racing Simulator (RARS) [Timin 1995]. This provides a dynamic, closed environment of a simulated racetrack. It provides a ready-made graphical output and an interface for "automobile robots". That is, it provides to each robot a set of parameters that describe the current environment, and receives settings f